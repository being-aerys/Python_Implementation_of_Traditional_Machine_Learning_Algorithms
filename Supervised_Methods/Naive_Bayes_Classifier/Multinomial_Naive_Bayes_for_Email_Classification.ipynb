{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes Classification\n",
    "This work is inspired from zacstewart.com and uses some code  and explanation from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custin train_test_split indices generator\n",
    "\n",
    "def train_test_split_indices(len_data, test_split_size = 0.3):\n",
    "    np.random.seed(42)\n",
    "    shuffled_data_indices = np.random.permutation(len_data)\n",
    "    total_test_data = int(test_split_size * len_data)\n",
    "    testing_indices = shuffled_data_indices[:total_test_data]\n",
    "    training_indices = shuffled_data_indices[total_test_data:]\n",
    "    \n",
    "    return training_indices, testing_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NEWLINE = '\\n'\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "#Read all files\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        #method walk() generates the file names in a directory tree\n",
    "        \n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build data frame from raw data\n",
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will build us a DataFrame from all the files in path. It will include the body text in one column and the class in another. Each row will be indexed by the corresponding email’s filename. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders_path = r\"C:\\Users\\Being_Aerys\\PycharmProjects\\Machine_Learning_Algorithms_Collection\\Supervised_Methods\\Naive_Bayes_Classifier\\Data\"\n",
    "#\\U in \"C:\\Users... starts an eight-character Unicode escape, such as \\U00014321. \n",
    "#The escape is followed by the character 's', which is invalid.\n",
    "#You either need to duplicate all backslashes.\n",
    "#r\"C:\\Users\\Eric\\Desktop\\beeline.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\being_aerys\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam',        SPAM),\n",
    "    ('data/spam_2',        SPAM),\n",
    "    ('data/easy_ham',    HAM),\n",
    "    ('data/easy_ham_2',    HAM),\n",
    "    ('data/hard_ham',    HAM),\n",
    "]\n",
    "\n",
    "data = DataFrame({'text': [], 'class': []})\n",
    "\n",
    "for path, classification in SOURCES:\n",
    "    data = data.append(build_data_frame(path, classification))\n",
    "\n",
    "#To shuffle the ham and spam indices\n",
    "data = data.reindex(np.random.permutation(data.index),)\n",
    "\n",
    "#add an index column to the data and set it as the index colmn\n",
    "data['index'] = range(0, len(data))\n",
    "data = data.set_index(\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the size of the training set is just a matter of dumping a collection of emails into a directory and then adding it to SOURCES with an applicable class. The last thing we do is use DataFrame’s reindex to shuffle the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>http://news.bbc.co.uk/1/hi/world/asia-pacific/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>What I understood was that the activists on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>URL: http://scriptingnews.userland.com/backiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>*****************BANNEDCD:::::::::::::::::::::...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               text\n",
       "index                                                         \n",
       "0      spam  1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...\n",
       "1       ham  http://news.bbc.co.uk/1/hi/world/asia-pacific/...\n",
       "2       ham  What I understood was that the activists on th...\n",
       "3       ham  URL: http://scriptingnews.userland.com/backiss...\n",
       "4      spam  *****************BANNEDCD:::::::::::::::::::::..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert labels from text values to 0 and 1 values\n",
    "data['class'] = data['class'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>http://news.bbc.co.uk/1/hi/world/asia-pacific/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>What I understood was that the activists on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>URL: http://scriptingnews.userland.com/backiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>*****************BANNEDCD:::::::::::::::::::::...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                               text\n",
       "index                                                          \n",
       "0          1  1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...\n",
       "1          0  http://news.bbc.co.uk/1/hi/world/asia-pacific/...\n",
       "2          0  What I understood was that the activists on th...\n",
       "3          0  URL: http://scriptingnews.userland.com/backiss...\n",
       "4          1  *****************BANNEDCD:::::::::::::::::::::..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_data = copy.deepcopy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer with stop words filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "all_features = count_vectorizer.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizer has learned the vocabulary of all the words in the email texts and also the count of each word in total in all the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(all_features, data['class'], test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, Y_train)\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Num of accurate predictions: 2725 and Num of inaccurate predictions 80\n"
     ]
    }
   ],
   "source": [
    "num_acccurate = (Y_test == predictions).sum()\n",
    "num_inaccurate = len(Y_test) - num_acccurate\n",
    "\n",
    "print(f\" Num of accurate predictions: {num_acccurate} and Num of inaccurate predictions {num_inaccurate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Percentage: 0.9714795008912656\n"
     ]
    }
   ],
   "source": [
    "accuracy_percent = classifier.score(X_test, Y_test)\n",
    "print(f\"Accuracy Percentage: {accuracy_percent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9730941704035875, Recall: 0.9130434782608695 f1_score: 0.9421128798842258\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "f1_scoree = f1_score(Y_test, predictions)\n",
    "print(f\"Precision: {precision}, Recall: {recall} f1_score: {f1_scoree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets do some ham/ spam classification on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following are some real ham/ spam samples from my own OSU email inbox.\n",
    "samples = [\n",
    "    \"Win $500 in 2 minutes View this email in the browser. We want to hear from you. Take our 1-2 minute OSU student survey. \",\n",
    "    \"CI Partnerships*Corrections* Dear Professor Adhikari, With reference to my previous email, we have discovered an error that may have\",\n",
    "    \"Sierra Live Construction Tour SIERRA SPIRIT TOUR for a $10 starbucks gift card.\",\n",
    "    \"Dear All I am trying to get a RAID controller for star. Currently it is using CentOS software RAID and that is unreliable. Please give me some time to obtain the controller.\",\n",
    "    \"Hi Aashish, The Sprintax team wishes you a happy Global Pride Day! We hope you enjoy a weekend filled with fun and activities. PS: don’t forget to prepare your taxes! The US tax deadline is just a couple of weeks away.\",\n",
    "    \"MyOregonState is a new digital experience just for you, one that's modern, friendly and personalized. It will be officially replacing MyOSU this summer — and this is just the beginning.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_samples = count_vectorizer.transform(samples)\n",
    "classifier.predict(vectorized_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three samples were from my spam folder while the rest three were from my regular inbox.\n",
    "The model correctly classified all the ham emails and missed just one spa email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer',  CountVectorizer(stop_words = \"english\")),\n",
    "    ('multinomial_NB_classifier',  MultinomialNB()) ])\n",
    "\n",
    "training_indices, testing_indices = train_test_split_indices(len(data), 0.3)\n",
    "\n",
    "pipeline.fit(data.iloc[training_indices]['text'], data.iloc[training_indices]['class'])\n",
    "pipeline_predictions = pipeline.predict(data.iloc[testing_indices]['text'])\n",
    "\n",
    "#Now calculate the performance metrices as above if you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 Score:, 0.9571283432540243\n",
      " Confusion Matrix: [[6925   26]\n",
      " [ 173 2225]]\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n_splits = 8, shuffle = False, random_state = 42)\n",
    "#returns training and testing indices iterator for k folds\n",
    "\n",
    "f1_scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(data):\n",
    "    train_text = data.iloc[train_indices]['text']\n",
    "    train_y = data.iloc[train_indices]['class']\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text']\n",
    "    test_y = data.iloc[test_indices]['class']\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "    \n",
    "    #We will consider each iteration of k-fold as a different set and thus add onto the previous confusion matrix\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    f1score = f1_score(test_y, predictions)\n",
    "    f1_scores.append(f1score)\n",
    "\n",
    "print(f\"f1 Score: {sum(f1_scores)/len(f1_scores)}\\n Confusion Matrix: {confusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Accuracy\n",
    "Above, we used Count Vectorization : Words from all the emails/ documents combinely formed a vocabulary. For each document/ training sample, each word acted as a feature of the document/ sample and we set that feature to 1 for this email/ document and set the remaining features/ words to 0. Thus this was an example of a count vectorization with unigram. i.e., we used a single word as one feature/ token. However, this can be changed such that we use two consecutive words (and thus take the order of the words into acccount as well unlike the unigram model above). This new method is called Count Vectorization with bigrams. In practice, using a bigram along with unigrams helps to improve accuracy in text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************Using both unigrams and bigrams*********************************\n",
      "With both unigrams and bigrams: \n",
      "f1 Score: 0.980260936850388\n",
      " Confusion Matrix: [[6931   20]\n",
      " [  74 2324]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"*********************************Using both unigrams and bigrams*********************************\")\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer',  CountVectorizer(stop_words = \"english\", ngram_range=(1, 2))),\n",
    "    ('multinomial_NB_classifier',  MultinomialNB()) ])\n",
    "\n",
    "training_indices, testing_indices = train_test_split_indices(len(data), 0.3)\n",
    "\n",
    "pipeline.fit(data.iloc[training_indices]['text'], data.iloc[training_indices]['class'])\n",
    "pipeline_predictions = pipeline.predict(data.iloc[testing_indices]['text'])\n",
    "\n",
    "\n",
    "k_fold = KFold(n_splits = 8, shuffle = False, random_state = 42)\n",
    "#returns training and testing indices iterator for k folds\n",
    "\n",
    "f1_scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(data):\n",
    "    train_text = data.iloc[train_indices]['text']\n",
    "    train_y = data.iloc[train_indices]['class']\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text']\n",
    "    test_y = data.iloc[test_indices]['class']\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "    \n",
    "    #We will consider each iteration of k-fold as a different set and thus add onto the previous confusion matrix\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    f1score = f1_score(test_y, predictions)\n",
    "    f1_scores.append(f1score)\n",
    "\n",
    "print(f\"With both unigrams and bigrams: \\nf1 Score: {sum(f1_scores)/len(f1_scores)}\\n Confusion Matrix: {confusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Exploration Topics:\n",
    "1.Bernoulli Naive Bayes with binarization threshold\n",
    "2.TF-IDF rather than a COunt Vectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
