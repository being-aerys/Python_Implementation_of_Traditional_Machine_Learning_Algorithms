{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes Classification\n",
    "This work is inspired from zacstewart.com and uses some methods and explanation from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load email texts and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NEWLINE = '\\n'\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "#Read all files\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        #method walk() generates the file names in a directory tree\n",
    "        \n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build data frame from raw data\n",
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will build us a DataFrame from all the files in path. It will include the body text in one column and the class in another. Each row will be indexed by the corresponding email’s filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders_path = r\"C:\\Users\\Being_Aerys\\PycharmProjects\\Machine_Learning_Algorithms_Collection\\Supervised_Methods\\Naive_Bayes_Classifier\\Data\"\n",
    "#\\U in \"C:\\Users... starts an eight-character Unicode escape, such as \\U00014321. \n",
    "#The escape is followed by the character 's', which is invalid.\n",
    "#You either need to duplicate all backslashes.\n",
    "#r\"C:\\Users\\Eric\\Desktop\\beeline.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\being_aerys\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/spam',        SPAM),\n",
    "    ('data/spam_2',        SPAM),\n",
    "    ('data/easy_ham',    HAM),\n",
    "    ('data/easy_ham_2',    HAM),\n",
    "    ('data/hard_ham',    HAM),\n",
    "#     ('data/beck-s',      HAM),\n",
    "#     ('data/farmer-d',    HAM),\n",
    "#     ('data/kaminski-v',  HAM),\n",
    "#     ('data/kitchen-l',   HAM),\n",
    "#     ('data/lokay-m',     HAM),\n",
    "#     ('data/williams-w3', HAM),\n",
    "#     ('data/BG',          SPAM),\n",
    "#     ('data/GP',          SPAM),\n",
    "#     ('data/SH',          SPAM)\n",
    "]\n",
    "\n",
    "data = DataFrame({'text': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data = data.append(build_data_frame(path, classification))\n",
    "\n",
    "#To shuffle the ham and spam indices\n",
    "data = data.reindex(np.random.permutation(data.index),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the size of the training set is just a matter of dumping a collection of emails into a directory and then adding it to SOURCES with an applicable class. The last thing we do is use DataFrame’s reindex to shuffle the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data/easy_ham\\00949.5a860f580179b99a227c4064ac28724c</th>\n",
       "      <td>ham</td>\n",
       "      <td>--==_Exmh_1581673767P\\n\\nContent-Type: text/pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/easy_ham\\2505.550c8b2240659bb8bf54b17edea4a96b</th>\n",
       "      <td>ham</td>\n",
       "      <td>URL: http://boingboing.net/#85538591\\n\\nDate: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/easy_ham_2\\01312.715fe5f6f31d47697d5a8f625fd2e49f</th>\n",
       "      <td>ham</td>\n",
       "      <td>From: Matt Kettler &lt;mkettler@evi-inc.com&gt;\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/spam_2\\01209.01df2f8f68a70062085ef787973f9ba0</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is a multipart message in MIME format.\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data/spam\\0128.4da9b2cfacbe9bfd128aacbb526d68d4</th>\n",
       "      <td>spam</td>\n",
       "      <td>&lt;html&gt;\\n\\n\\n\\n&lt;head&gt;\\n\\n&lt;meta http-equiv=3D\"Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   class  \\\n",
       "data/easy_ham\\00949.5a860f580179b99a227c4064ac2...   ham   \n",
       "data/easy_ham\\2505.550c8b2240659bb8bf54b17edea4...   ham   \n",
       "data/easy_ham_2\\01312.715fe5f6f31d47697d5a8f625...   ham   \n",
       "data/spam_2\\01209.01df2f8f68a70062085ef787973f9ba0  spam   \n",
       "data/spam\\0128.4da9b2cfacbe9bfd128aacbb526d68d4     spam   \n",
       "\n",
       "                                                                                                 text  \n",
       "data/easy_ham\\00949.5a860f580179b99a227c4064ac2...  --==_Exmh_1581673767P\\n\\nContent-Type: text/pl...  \n",
       "data/easy_ham\\2505.550c8b2240659bb8bf54b17edea4...  URL: http://boingboing.net/#85538591\\n\\nDate: ...  \n",
       "data/easy_ham_2\\01312.715fe5f6f31d47697d5a8f625...  From: Matt Kettler <mkettler@evi-inc.com>\\n\\n\\...  \n",
       "data/spam_2\\01209.01df2f8f68a70062085ef787973f9ba0  This is a multipart message in MIME format.\\n\\...  \n",
       "data/spam\\0128.4da9b2cfacbe9bfd128aacbb526d68d4     <html>\\n\\n\\n\\n<head>\\n\\n<meta http-equiv=3D\"Co...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert labels from text values to 0 and 1 values\n",
    "data['class'] = data['class'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "all_features = count_vectorizer.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizer has learned the vocabulary of all the words in the email texts and also the count of each word in total in all the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(all_features, data['class'], test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Num of accurate predictions: 2751 and Num of inaccurate predictions 54\n"
     ]
    }
   ],
   "source": [
    "classifier = MultinomialNB() #understand why using a multinomial Naive Bayes\n",
    "classifier.fit(X_train, Y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "num_acccurate = (Y_test == predictions).sum()\n",
    "num_inaccurate = len(Y_test) - num_acccurate\n",
    "\n",
    "print(f\" Num of accurate predictions: {num_acccurate} and Num of inaccurate predictions {num_inaccurate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Percentage: 0.9807486631016042\n"
     ]
    }
   ],
   "source": [
    "accuracy_percent = classifier.score(X_test, Y_test)\n",
    "print(f\"Accuracy Percentage: {accuracy_percent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work on precision recall f1 score, other forms of vectorizer, prior that couldve been used, etc now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
