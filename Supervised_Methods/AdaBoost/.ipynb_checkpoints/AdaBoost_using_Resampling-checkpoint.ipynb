{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost for Optical Character Recognition\n",
    "We train a bunch of weak classifiers according to Adaptive Bootstrapping (Adaboost). The weak classifiers used here are decision trees with shallow depth but can be replaced with other weak classifiers as well. We classify between two handwritten numbers, 3 and 5. The data set used is from Oregon State University, Fall 2018 CS 534 Machine Learning Course Implementation Assignment 3.\n",
    "\n",
    "There are 100 features for each sample, corresponding to the top 100 principal components generated using PCA from the original sample features of size 28 * 28.\n",
    "\n",
    "The training set contains 4888 samples. Each sample is a list of 101 values. The first column represents the digitâ€™s label which is 3 or 5. The other 100 floating values are the PCA-generated features for this sample.\n",
    "\n",
    "The test set contains 1629 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, raw_training_data_path, raw_validation_data_path):\n",
    "            \n",
    "            # Read the training and validation data. As the features are just PCA components, no need to observe the \n",
    "            # column names, so can proceed with numpy instead of pandas.\n",
    "            \n",
    "            self.raw_training_data = np.genfromtxt(raw_training_data_path, delimiter = \",\")\n",
    "            self.raw_validation_data = np.genfromtxt(raw_validation_data_path, delimiter = \",\")\n",
    "            \n",
    "            print(self.raw_training_data[0:10,:])\n",
    "            \n",
    "            # split training features and labels\n",
    "            self.training_features = self.raw_training_data[:, 1:]\n",
    "            self.training_labels = self.raw_training_data[:, 0]\n",
    "            \n",
    "            print(\"No of training samples: {}, No of training features: {}\".format(len(self.training_labels), self.training_features.shape[1]))\n",
    "            \n",
    "            \n",
    "            # split validation features and labels\n",
    "            self.validation_features = self.raw_validation_data[:, 1:]\n",
    "            self.validation_labels = self.raw_validation_data[:, 0]\n",
    "            \n",
    "            print(\"No of validation samples: {}\".format(len(self.validation_labels), ))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree_Node:\n",
    "    \n",
    "    def __init__(self, X = None, y = None, depth = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.depth = depth\n",
    "        self.feature_index_with_optimal_split = None\n",
    "        self.threshold_for_optimal_split = None # since features are real values\n",
    "        self.is_leaf = False\n",
    "        self.class_probabilities = None\n",
    "\n",
    "\n",
    "class Stump_Decision_Tree(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, root = None, max_depth = None, min_pool = None, weights_of_training_samples = None):\n",
    "        '''\n",
    "        Implementation of a stump decision tree i.e., a decision tree with a root and a pair of left and right children.\n",
    "\n",
    "        Parameters\n",
    "        -------------\n",
    "        max_depth : int, optional, default 1\n",
    "                    The maximum depth of the deicison tree. For a stump learner, by default 1.\n",
    "        min_pool :  int, optional, default +INF\n",
    "                    The minimum number of samples at a node after split. For a stump learner, no bound required.\n",
    "        '''\n",
    "        self.root = root\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_pool\n",
    "        self.classes = None\n",
    "        self.weights_of_training_samples = weights_of_training_samples\n",
    "    \n",
    "    \n",
    "    def build_decision_tree(self, X, y, node, current_depth):\n",
    "\n",
    "        '''\n",
    "        Recursively builds the decision tree if the terminal conditions not met\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        X : training features\n",
    "        y : training labels\n",
    "        ''' \n",
    "        \n",
    "        # base cases of recursion\n",
    "        if current_depth >= self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "            \n",
    "        elif len(y) <= self.min_samples:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "        \n",
    "        elif len(np.unique(y)) == 1:\n",
    "            node.is_leaf = True\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            \n",
    "            \n",
    "            # find the best split and the feature that gives this best split\n",
    "            feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right = self.best_split(X, y)#, node.depth) \n",
    "            \n",
    "            \n",
    "            if feature_for_best_split is None:\n",
    "                # best_split() returns None if no feature gave a split that has less impurity than the parent\n",
    "                node.is_leaf = True\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Recursively build trees to the left and the right using the best features and the best split threshold\n",
    "                node.depth = current_depth\n",
    "                node.feature_index_with_optimal_split = feature_for_best_split\n",
    "                node.threshold_for_optimal_split = best_split_threshold\n",
    "                \n",
    "                node.left = Decision_Tree_Node(X_left, y_left, current_depth+1)\n",
    "                node.right = Decision_Tree_Node(X_right, y_right, current_depth+1)\n",
    "                \n",
    "                # when a node is encountered, calculate the class probabilities for each class at this node and store\n",
    "                node.left.class_probabilities = self.calculate_class_probabilities(y_left)\n",
    "                node.right.class_probabilities = self.calculate_class_probabilities(y_right)\n",
    "                \n",
    "                self.build_decision_tree(X_left, y_left, node.left, current_depth+1)\n",
    "                self.build_decision_tree(X_right, y_right, node.right, current_depth+1)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def gini_data(self,y): #####################MAKE THIS EFFICEINT\n",
    "        '''\n",
    "        Calculates the GINI impurity of y before split.\n",
    "        '''\n",
    "        #count the Y equal to one\n",
    "        c_root_pos=np.count_nonzero(y==3) # The labels are 3 and 5, 3 is the first label in ascending order.\n",
    "        c_root_total = len(y)        \n",
    "        \n",
    "        gini_of_y =  1- (c_root_pos/c_root_total)**2 -((c_root_total-c_root_pos)/c_root_total)**2 \n",
    "        return gini_of_y\n",
    "  \n",
    "\n",
    "    def gini_after_split(self, y_copy, indices_where_change, i):\n",
    "        \n",
    "        '''\n",
    "        Calculates GINI impurity metric U after split of y_copy.\n",
    "        \n",
    "        U = (p^2) + (q^2 )\n",
    "        \n",
    "        where p = No of samples at this node with feature x below the node threshold/ No of samples at this node\n",
    "              q = No of samples at this node with feature x equal or above the node threshold/ No of samples at this node\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        '''\n",
    "        \n",
    "        CL_pos = np.count_nonzero(y_copy[:indices_where_change[i] + 1] == 3)\n",
    "        CL_total = len(y_copy[:indices_where_change[i] + 1])\n",
    "        CL_neg = CL_total - CL_pos\n",
    "        prob1 = CL_pos / (CL_total)\n",
    "        prob2 = CL_neg / (CL_total)\n",
    "        UAL = 1 - (prob1) ** 2 - (prob2) ** 2\n",
    "\n",
    "\n",
    "\n",
    "        CR_pos = np.count_nonzero(y_copy[indices_where_change[i] + 1:] == 3)\n",
    "        CR_total = len(y_copy[indices_where_change[i] + 1:])\n",
    "        CR_neg = CR_total - CR_pos\n",
    "        prob1 = CR_pos / (CR_total)\n",
    "        prob2 = CR_neg / (CR_total)\n",
    "        UAR = 1 - (prob1) ** 2 - (prob2) ** 2\n",
    "\n",
    "\n",
    "        pl = (CL_total) / len(y_copy)\n",
    "        pr = (CR_total) / len(y_copy)\n",
    "        \n",
    "        gini_after_split =  pl * UAL + pr * UAR\n",
    "        \n",
    "        return gini_after_split\n",
    "    \n",
    "    def split_node(self, X, y, best_feature, max_benefit_val):\n",
    "        '''\n",
    "        Split a node using the best feature that maximizes Information Gain.\n",
    "        '''\n",
    "        left_X, right_X, left_y, right_y = [],[],[],[]\n",
    "\n",
    "        for row in range(0,X.shape[0]):\n",
    "            if X[row][best_feature] < max_benefit_val:\n",
    "                left_X.append(X[row])\n",
    "                left_y.append(y[row])\n",
    "            else:\n",
    "                right_X.append(X[row])\n",
    "                right_y.append(y[row])\n",
    "        \n",
    "        return np.array(left_X), np.array(right_X), np.array(left_y), np.array(right_y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def best_split(self, X_passed, y_passed):\n",
    "        '''\n",
    "        Divides the training samples into optimal splits according to the impurity metric\n",
    "        '''\n",
    "        numpy_X = []\n",
    "        numpy_y = []\n",
    "        \n",
    "        for row in range(0,len(X_passed)):\n",
    "            numpy_X.append(X_passed[row])\n",
    "            numpy_y.append(y_passed[row])\n",
    "        \n",
    "        X = np.array(numpy_X)\n",
    "        y = np.array(numpy_y)\n",
    "        \n",
    "        \n",
    "        gini_before_split = self.gini_data(y)\n",
    "\n",
    "        benefit_of_split = 0\n",
    "        \n",
    "        benefit_vals_for_one_feature = np.zeros(X.shape[1])\n",
    "        max_benefit_index_for_each_feature = np.zeros(X.shape[1])\n",
    "\n",
    "        \n",
    "        # for each feature, find the GINI gain at each threshold\n",
    "        for feature in range(0,X.shape[1]):\n",
    "            \n",
    "            current_feature = X[:, feature]\n",
    "            \n",
    "            # TRICK: Sort the training samples according to this feature's values\n",
    "            # and find GINI only at samples where the class label changes.\n",
    "            \n",
    "            \n",
    "            stacked = np.column_stack((y, current_feature)) # stack y_to_take and current_feature\n",
    "            sortedd = stacked[np.argsort(stacked[:, 1])]\n",
    "            \n",
    "            y_copy = sortedd[..., 0]\n",
    "            #indices_where_change = np.where(np.roll(y_copy,1)!=y_copy)[0]\n",
    "\n",
    "            indices_where_change = np.where(y_copy[:-1] != y_copy[1:])[0]\n",
    "            \n",
    "            # print(\"indices_where_change \",len(indices_where_change))\n",
    "            # the values in indices_where_change are the indices after which the label changes.\n",
    "                                            \n",
    "            for i in range(len(indices_where_change)):\n",
    "\n",
    "                gini_after_split = self.gini_after_split(y_copy, indices_where_change, i)\n",
    "\n",
    "                benefit_of_split = gini_before_split - gini_after_split\n",
    "\n",
    "\n",
    "                if benefit_of_split >  benefit_vals_for_one_feature[feature]:\n",
    "                    benefit_vals_for_one_feature[feature] = benefit_of_split  \n",
    "                    max_benefit_index_for_each_feature[feature] = indices_where_change[i]\n",
    "                   \n",
    "                \n",
    "                    \n",
    "\n",
    "        if benefit_of_split == 0:\n",
    "            #print(\"No good feature found.\")\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            \n",
    "            best_feature = np.argmax(benefit_vals_for_one_feature)\n",
    "            index_of_threhold_with_max_benefit_for_best_feature = max_benefit_index_for_each_feature[best_feature]\n",
    "            \n",
    "            # lets get the column of that feature and sort and get the threshold\n",
    "            max_benefit_feature = copy.deepcopy(X[:,best_feature])\n",
    "            max_benefit_feature.sort()\n",
    "            \n",
    "            # Athough we have threshold_index_for_best_split_in_indices_where_change, we need to find the\n",
    "            # corresponding index in the sorted max benefit feature.\n",
    "            \n",
    "\n",
    "            max_benefit_val = max_benefit_feature[int(index_of_threhold_with_max_benefit_for_best_feature)+1]\n",
    "            \n",
    "            \n",
    "            X_left, X_right, y_left, y_right = self.split_node(X, y, best_feature, max_benefit_val)\n",
    "            \n",
    "                        \n",
    "            # feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right\n",
    "            return best_feature, max_benefit_val, X_left, X_right, y_left, y_right\n",
    "                \n",
    "                \n",
    "                        \n",
    "\n",
    "                \n",
    "    def calculate_class_probabilities(self, y):\n",
    "        '''\n",
    "        Calculates the probability of each class at this node\n",
    "        '''\n",
    "        \n",
    "        class_probabilities = []\n",
    "        \n",
    "        for label in self.root.classes:\n",
    "            \n",
    "            if len(y[y==label]) == len(y) :\n",
    "                class_prob = 1\n",
    "            elif len(y[y==label]) == 0:\n",
    "                class_prob = 0\n",
    "            else:\n",
    "                class_prob = y[y==label].shape[0]/len(y)\n",
    "            \n",
    "            class_probabilities.append(class_prob)\n",
    "        \n",
    "        return class_probabilities\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    def predict_a_sample(self, x, node):\n",
    "        '''\n",
    "        Predicts a class label for the passed test sample by recursively calling a child node from the root upto\n",
    "        a leaf node of the learned decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        x : a single test sample features\n",
    "        node : a node of the learned decision tree\n",
    "        '''\n",
    "        \n",
    "        if node.is_leaf:\n",
    "            return node.class_probabilities\n",
    "        else:\n",
    "            \n",
    "            if x[node.feature_index_with_optimal_split] < node.threshold_for_optimal_split:\n",
    "                return self.predict_a_sample(x, node.left)\n",
    "            else:\n",
    "                return self.predict_a_sample(x, node.right)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts class labels for all test samples passed.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------------------\n",
    "        X : test samples passed as 2D arrays\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        test_preds = []\n",
    "        for test_sample in X:\n",
    "            class_probabilities = self.predict_a_sample(test_sample, self.root)\n",
    "            test_pred = np.argmax(class_probabilities)\n",
    "            if test_pred == 0:\n",
    "                \n",
    "                test_preds.append(3)\n",
    "            elif test_pred == 1:\n",
    "                test_preds.append(5)\n",
    "            else:\n",
    "                raise AssertionError\n",
    "        \n",
    "        return test_preds\n",
    "        \n",
    "       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        '''\n",
    "        Train CART decision tree using GINI impurity.\n",
    "        \n",
    "        Parameters\n",
    "        ----------------\n",
    "        X : arraylike\n",
    "            The training samples in 2D format, corresponding to input features of each sample.\n",
    "        y : arraylike, optional, default None\n",
    "            The labels for each sample in X. Some algorithms do not require the label y while some (like Decisoin Tree) do.\n",
    "            Hence, keep y as optional.\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        self : Trained Decision Tree object\n",
    "               This method returns self for compatibility reasons with sklearn's other interfaces/ functionalities.\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.root = Decision_Tree_Node(X, y, 0)\n",
    "        self.root.classes = list(set(y)) # gives the set of unique elements in y.\n",
    "        self.root.classes.sort() # sorts in ascending order in-place if not already sorted\n",
    "        \n",
    "        #print(\"The unique classes present in training data:{}\".format(self.root.classes))\n",
    "        \n",
    "        self.build_decision_tree(X = X, y = y, node = self.root, current_depth = 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost can be implemented using decision trees in two ways to ensure proper weightage of all the sample.<br>\n",
    "1. Use weighted gini index using sample weights  Or,\n",
    "2. For each new weak learner, sample a new data set from the old one proportional to the updated sample weights. (As in this file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self, rounds_of_boosting):\n",
    "        \n",
    "        self.train_X = None\n",
    "        self.train_y = None\n",
    "        self.test_X = None\n",
    "        self.text_y = None\n",
    "        \n",
    "        self.sample_weights = []\n",
    "        \n",
    "        self.rounds_of_boosting = rounds_of_boosting\n",
    "        self.random_forest = [] # adaboost maintains a forest of weak learners similar to random forest.\n",
    "        self.weights_of_learners = []\n",
    "    \n",
    "    \n",
    "    def calculate_learner_weight(self, total_error):\n",
    "        '''\n",
    "        Calculates alpha parameter that decides the weight of each learner in the final vote.\n",
    "        '''\n",
    "        if not(0 <= total_error and total_error <= 1.1): # theoretically, should be 1.\n",
    "            print(\"Total error\", total_error)\n",
    "            raise AssertionError\n",
    "        \n",
    "        weight_of_learner = 0.5 * ( math.log( (1-total_error + 0.00000000000001)/(total_error + 0.00000000000001) ) )\n",
    "        \n",
    "        if total_error < 0.5:\n",
    "            assert weight_of_learner > 0\n",
    "        \n",
    "        return weight_of_learner\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update_sample_weights(self, learner_weight, old_sample_weights, individual_unweighted_errors):\n",
    "        '''\n",
    "        Updates the weights of individual samples based on the error made by the learner.\n",
    "        '''\n",
    "        unnormalized_updated_sample_weights = []\n",
    "        \n",
    "        for sample_weight_idx in range(0,len(old_sample_weights)):\n",
    "            \n",
    "            if individual_unweighted_errors[sample_weight_idx] == 0:\n",
    "                e_power_multiplier = math.exp(-learner_weight)\n",
    "            elif individual_unweighted_errors[sample_weight_idx] == 1:\n",
    "                e_power_multiplier = math.exp(learner_weight)\n",
    "            else:\n",
    "                \n",
    "                raise ValueError\n",
    "            \n",
    "            unnormalized_updated_sample_weights.append(old_sample_weights[sample_weight_idx] * e_power_multiplier)\n",
    "        \n",
    "        total_weight = sum(unnormalized_updated_sample_weights)\n",
    "        normalized_updated_sample_weights = [item/total_weight for item in unnormalized_updated_sample_weights]\n",
    "        \n",
    "        return normalized_updated_sample_weights\n",
    "                \n",
    "            \n",
    "            \n",
    "    def calculate_weighted_error(self, X, y, weights_of_samples, learner):\n",
    "        '''\n",
    "        Calculates the weighted error made on each sample by the learner.\n",
    "        '''\n",
    "        predictions = learner.predict(X)\n",
    "        individual_unweighted_errors = []\n",
    "        sum_of_weighted_errors = 0\n",
    "        for sample in range(len(predictions)):\n",
    "            \n",
    "            if y[sample] != predictions[sample]:\n",
    "                sum_of_weighted_errors += weights_of_samples[sample]\n",
    "                individual_unweighted_errors.append(1)\n",
    "            else:\n",
    "                individual_unweighted_errors.append(0)\n",
    "        \n",
    "        assert 0 <= sum_of_weighted_errors and sum_of_weighted_errors <= 1.1\n",
    "        \n",
    "        return sum_of_weighted_errors, individual_unweighted_errors\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def update_training_samples(self, X,y, current_sample_weights):\n",
    "        '''\n",
    "        Creates a new training set by sampling from the old training set with replacement proportional to the sample weights.\n",
    "        '''\n",
    "        assert sum(current_sample_weights) <= 1.1 # Theoretically, <= 1 but need to address precision issues.\n",
    "        \n",
    "        # generate a cumulative array of sample weights\n",
    "        cumulative_sample_weights = []\n",
    "        total = 0\n",
    "        for idx in range(0,len(current_sample_weights)):\n",
    "            total = total+current_sample_weights[idx]\n",
    "            cumulative_sample_weights.append(total)\n",
    "        \n",
    "       \n",
    "        new_X = []\n",
    "        new_y = []\n",
    "        \n",
    "        for _ in range(len(y)):\n",
    "            \n",
    "            # sample a random number between 0 and 1\n",
    "            random_val = random.uniform(0,1)\n",
    "            \n",
    "            # find at which index in the cumulative array this random value lies\n",
    "            for idx in range(0, len(current_sample_weights)):\n",
    "                \n",
    "                if cumulative_sample_weights[idx] >= random_val:\n",
    "                    new_X.append(X[idx])\n",
    "                    new_y.append(y[idx])\n",
    "                    break\n",
    "        \n",
    "        assert len(new_X) == len(X)\n",
    "        \n",
    "        return new_X, new_y\n",
    "            \n",
    "       \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts the labels for different samples in X using majority vote of different weak learners.\n",
    "        '''\n",
    "        \n",
    "        predictions_from_different_learners = []\n",
    "        \n",
    "        for idx in range(self.rounds_of_boosting):\n",
    "            \n",
    "            # make predictions for all the samples in X using this learner\n",
    "            predictions = self.random_forest[idx].predict(X)\n",
    "            predictions_from_different_learners.append(predictions)\n",
    "            \n",
    "        predictions_from_different_learners = np.array(predictions_from_different_learners)\n",
    "        # Each row contains the predictions for different samples from one learner. \n",
    "        \n",
    "        \n",
    "        predictions_from_different_learners = predictions_from_different_learners.T\n",
    "        # Each row contains the predictions for one sample from different learners. \n",
    "        \n",
    "        # for all samples in X, vote using all the learners\n",
    "        final_predictions = []\n",
    "        \n",
    "        for sample_idx in range(len(X)):\n",
    "            \n",
    "            cumulative_prediction_for_sample = 0\n",
    "            for learner_no in range(self.rounds_of_boosting):\n",
    "                \n",
    "                \n",
    "                \n",
    "                if predictions_from_different_learners[sample_idx][learner_no] == 3:\n",
    "                    cumulative_prediction_for_sample += (self.weights_of_learners[learner_no] )\n",
    "                elif predictions_from_different_learners[sample_idx][learner_no] == 5:\n",
    "                    cumulative_prediction_for_sample -= (self.weights_of_learners[learner_no] )\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            \n",
    "            final_prediction = None\n",
    "            if cumulative_prediction_for_sample >= 0:\n",
    "                final_prediction = 3\n",
    "                \n",
    "            else:\n",
    "                final_prediction = 5\n",
    "                \n",
    "            final_predictions.append(final_prediction)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return final_predictions\n",
    "                     \n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self, X ,y):\n",
    "        '''\n",
    "        Trains an Adaboost instance.\n",
    "        '''\n",
    "        self.train_X = X\n",
    "        self.train_y = y\n",
    "        \n",
    "        leng_of_training_set = len(X)\n",
    "        \n",
    "        current_sample_weights = [1/leng_of_training_set for _ in range(leng_of_training_set)]\n",
    "        self.sample_weights.append(current_sample_weights)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for weak_learner_num in range(0,self.rounds_of_boosting):\n",
    "            print(\"\\nWeak Learner: {} is training.\".format(weak_learner_num))\n",
    "            \n",
    "            # train a weak learner\n",
    "            decision_tree_root = Decision_Tree_Node()\n",
    "            weak_learner = Stump_Decision_Tree(root = decision_tree_root, max_depth = 1, min_pool = 1, weights_of_training_samples = current_sample_weights)\n",
    "            weak_learner.fit(self.train_X, self.train_y)\n",
    "            self.random_forest.append(weak_learner)\n",
    "            \n",
    "            # calculate weighted error\n",
    "            sum_of_weighted_errors, individual_unweighted_errors = self.calculate_weighted_error(self.train_X, self.train_y, current_sample_weights, weak_learner)\n",
    "            \n",
    "            # update alpha for this learner\n",
    "            weight_of_learner = self.calculate_learner_weight(sum_of_weighted_errors)\n",
    "            \n",
    "            self.weights_of_learners.append(weight_of_learner)\n",
    "            \n",
    "            # Update the sample weights for the training samples\n",
    "            current_sample_weights = self.update_sample_weights(weight_of_learner, current_sample_weights, individual_unweighted_errors)\n",
    "        \n",
    "            \n",
    "            # Update the training samples using Sampling with replacement\n",
    "            self.train_X, self.train_y = self.update_training_samples(self.train_X, self.train_y, current_sample_weights)\n",
    "            \n",
    "            \n",
    "            # save the updated weights\n",
    "            if weak_learner_num+1 < self.rounds_of_boosting:\n",
    "                self.sample_weights.append(current_sample_weights)\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        print(\"The voting weights of different learners of Adaboost forest: \",self.weights_of_learners)    \n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5.     -252.5092 1040.6188 ...  -21.6094  -32.602    25.619 ]\n",
      " [   5.     -684.5502 -368.7191 ...  -36.3467   26.6937  -17.564 ]\n",
      " [   3.      119.2934  638.9885 ...   14.7913   48.7926  -94.5664]\n",
      " ...\n",
      " [   5.      972.0162   77.9232 ...  -35.5166  -16.6162  -43.1298]\n",
      " [   5.     -255.5209  144.6523 ...   75.572    34.6369   24.2973]\n",
      " [   3.      217.1434  592.4619 ...  -25.7318   55.1806    4.9309]]\n",
      "No of training samples: 4888, No of training features: 100\n",
      "No of validation samples: 1629\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "training_data_path = \"data/pa3_train_reduced.csv\"\n",
    "validation_data_path = \"data/pa3_valid_reduced.csv\"\n",
    "\n",
    "preprocessing = Preprocessing(training_data_path, validation_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 1\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592]\n",
      "Training Accuracy : 0.7371112929623568 Testing Accuracy : 0.7286678944137508\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 2\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.5749081140019153]\n",
      "Training Accuracy : 0.6884206219312602 Testing Accuracy : 0.7016574585635359\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 3\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.5863717161814763, 0.4407529282722803]\n",
      "Training Accuracy : 0.8050327332242226 Testing Accuracy : 0.8078575813382444\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 4\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.5683762335503428, 0.3339698727121321, 0.362817073850335]\n",
      "Training Accuracy : 0.8211947626841244 Testing Accuracy : 0.8201350521792511\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 5\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.608875653718329, 0.3882405656124485, 0.46344572863948585, 0.33542555111946926]\n",
      "Training Accuracy : 0.8379705400981997 Testing Accuracy : 0.8360957642725598\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 6\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.5632702423550143, 0.3548699670637973, 0.46430591777991864, 0.3895574272729286, 0.22632818873417654]\n",
      "Training Accuracy : 0.824468085106383 Testing Accuracy : 0.8189073050951504\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 7\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "\n",
      "Weak Learner: 6 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.6171282768184981, 0.39054743099596556, 0.4577988232596064, 0.3763276660896826, 0.34308031331080685, 0.2879722365264396]\n",
      "Training Accuracy : 0.824468085106383 Testing Accuracy : 0.8195211786372008\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 8\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "\n",
      "Weak Learner: 6 is training.\n",
      "\n",
      "Weak Learner: 7 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.541219178142116, 0.4020015307526103, 0.41267943966477527, 0.3589923240801343, 0.3050164119897603, 0.4184788574150057, 0.4954466374013281]\n",
      "Training Accuracy : 0.8492225859247136 Testing Accuracy : 0.856353591160221\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 9\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "\n",
      "Weak Learner: 6 is training.\n",
      "\n",
      "Weak Learner: 7 is training.\n",
      "\n",
      "Weak Learner: 8 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.5930206476315343, 0.35550873762094143, 0.36617709665493503, 0.4462807400193334, 0.3337560376797676, 0.35426019579428597, 0.32378366935100955, 0.2934185816697606]\n",
      "Training Accuracy : 0.8711129296235679 Testing Accuracy : 0.8680171884591774\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 10\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "\n",
      "Weak Learner: 6 is training.\n",
      "\n",
      "Weak Learner: 7 is training.\n",
      "\n",
      "Weak Learner: 8 is training.\n",
      "\n",
      "Weak Learner: 9 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.5734046918387331, 0.3607789352956006, 0.445189307632445, 0.19474313084895084, 0.43275160134707563, 0.3586787515550499, 0.33904055441502795, 0.2992220551655829, 0.3200308862466728]\n",
      "Training Accuracy : 0.8479950900163666 Testing Accuracy : 0.8268876611418048\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 11\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "\n",
      "Weak Learner: 6 is training.\n",
      "\n",
      "Weak Learner: 7 is training.\n",
      "\n",
      "Weak Learner: 8 is training.\n",
      "\n",
      "Weak Learner: 9 is training.\n",
      "\n",
      "Weak Learner: 10 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.6144832012081881, 0.3802532013050514, 0.3939306543478879, 0.2970348300026836, 0.2127464209913147, 0.33712145958910594, 0.2952142481078549, 0.3589474688857794, 0.3513997153473121, 0.42639940527000864]\n",
      "Training Accuracy : 0.8668166939443536 Testing Accuracy : 0.8594229588704727\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 12\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "\n",
      "Weak Learner: 6 is training.\n",
      "\n",
      "Weak Learner: 7 is training.\n",
      "\n",
      "Weak Learner: 8 is training.\n",
      "\n",
      "Weak Learner: 9 is training.\n",
      "\n",
      "Weak Learner: 10 is training.\n",
      "\n",
      "Weak Learner: 11 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.5654363529716263, 0.35810084164688283, 0.41941777628870236, 0.36484867979421737, 0.38161100320376945, 0.2135920097938178, 0.3622261669904637, 0.21323196603725586, 0.43306203911269253, 0.3353174545112127, 0.18881698499311675]\n",
      "Training Accuracy : 0.8719312602291326 Testing Accuracy : 0.852670349907919\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 13\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "\n",
      "Weak Learner: 6 is training.\n",
      "\n",
      "Weak Learner: 7 is training.\n",
      "\n",
      "Weak Learner: 8 is training.\n",
      "\n",
      "Weak Learner: 9 is training.\n",
      "\n",
      "Weak Learner: 10 is training.\n",
      "\n",
      "Weak Learner: 11 is training.\n",
      "\n",
      "Weak Learner: 12 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.5943273016825608, 0.4090004004709701, 0.36647509795910715, 0.3598174706340688, 0.3793971008473844, 0.3503309512995957, 0.2604068729433454, 0.3619724920740469, 0.38934427251530507, 0.26023770581512523, 0.369660972437688, 0.2989991508103209]\n",
      "Training Accuracy : 0.8827741407528642 Testing Accuracy : 0.8747697974217311\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 14\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "\n",
      "Weak Learner: 6 is training.\n",
      "\n",
      "Weak Learner: 7 is training.\n",
      "\n",
      "Weak Learner: 8 is training.\n",
      "\n",
      "Weak Learner: 9 is training.\n",
      "\n",
      "Weak Learner: 10 is training.\n",
      "\n",
      "Weak Learner: 11 is training.\n",
      "\n",
      "Weak Learner: 12 is training.\n",
      "\n",
      "Weak Learner: 13 is training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.5994951673476523, 0.386991345328971, 0.41917846070914905, 0.30507110953182326, 0.34358212474855404, 0.3382837070064667, 0.2588388567780027, 0.3108859519904564, 0.2554608738304645, 0.24193077476046407, 0.3899983520237478, 0.3496487150612503, 0.2450464645898516]\n",
      "Training Accuracy : 0.886252045826514 Testing Accuracy : 0.8723143032535298\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 15\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "\n",
      "Weak Learner: 6 is training.\n",
      "\n",
      "Weak Learner: 7 is training.\n",
      "\n",
      "Weak Learner: 8 is training.\n",
      "\n",
      "Weak Learner: 9 is training.\n",
      "\n",
      "Weak Learner: 10 is training.\n",
      "\n",
      "Weak Learner: 11 is training.\n",
      "\n",
      "Weak Learner: 12 is training.\n",
      "\n",
      "Weak Learner: 13 is training.\n",
      "\n",
      "Weak Learner: 14 is training.\n",
      "The voting weights of different learners of Adaboost forest:  [0.5155040567093592, 0.5976167639265906, 0.4258119207677476, 0.49881790349960187, 0.4034984049985212, 0.22475074853391586, 0.3232577120811889, 0.38728674074441055, 0.3478327813518476, 0.3304399299116511, 0.32017639490291505, 0.3407911745005034, 0.261119892982385, 0.2761021501007131, 0.5323059257547308]\n",
      "Training Accuracy : 0.8608837970540099 Testing Accuracy : 0.8434622467771639\n"
     ]
    }
   ],
   "source": [
    "# Observe the trend for different no of trees in the adaboost forest.\n",
    "no_of_boosting_rounds_list = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for no_of_boosting_rounds in no_of_boosting_rounds_list:\n",
    "    print(\"***************************************************\")\n",
    "    print(\"\\nNo of boosting rounds for current Adaboost: {}\".format(no_of_boosting_rounds))\n",
    "    adaboost_instance = AdaBoost(rounds_of_boosting = no_of_boosting_rounds)\n",
    "    X, y = copy.deepcopy(preprocessing.training_features), copy.deepcopy(preprocessing.training_labels)\n",
    "    adaboost_instance.fit(X, y)\n",
    "    \n",
    "    \n",
    "    # predict for both training data and test data\n",
    "    training_predictions = adaboost_instance.predict(preprocessing.training_features)\n",
    "    training_acc = accuracy_score(preprocessing.training_labels, training_predictions)\n",
    "    training_accuracy.append(training_acc)\n",
    "    \n",
    "    \n",
    "    test_predictions = adaboost_instance.predict(preprocessing.validation_features)\n",
    "    test_acc = accuracy_score(preprocessing.validation_labels, test_predictions)\n",
    "    test_accuracy.append(test_acc)\n",
    "    print(\"Training Accuracy : {} Testing Accuracy : {}\".format(training_acc, test_acc))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(no_of_boosting_rounds_list, training_accuracy, 'g--', label=\"Training Accuracy\")\n",
    "plt.plot(no_of_boosting_rounds_list, test_accuracy, 'r--', label=\"Test Accuracy\")\n",
    "plt.xlabel(\"No of Trees in Random Forest\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training set contains significant noise (or  mislabelled samples), with more number of iterations, adaboost can give more importance to the noisy samples (or  mislabelled samples) and thus result in overfitting to consequently decrease the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
