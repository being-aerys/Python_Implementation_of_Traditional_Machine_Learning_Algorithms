{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost for Optical Character Recognition\n",
    "We train a bunch of weak classifiers according to Adaptive Bootstrapping (Adaboost). The weak classifiers used here are decision trees with shallow depth but can be replaced with other weak classifiers as well. We classify between two handwritten numbers, 3 and 5. The data set used is from Oregon State University, Fall 2018 CS 534 Machine Learning Course Implementation Assignment 3.\n",
    "\n",
    "There are 100 features for each sample, corresponding to the top 100 principal components generated using PCA from the original sample features of size 28 * 28.\n",
    "\n",
    "The training set contains 4888 samples. Each sample is a list of 101 values. The first column represents the digitâ€™s label which is 3 or 5. The other 100 floating values are the PCA-generated features for this sample.\n",
    "\n",
    "The test set contains 1629 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, raw_training_data_path, raw_validation_data_path):\n",
    "            \n",
    "            # Read the training and validation data. As the features are just PCA components, no need to observe the \n",
    "            # column names, so can proceed with numpy instead of pandas.\n",
    "            \n",
    "            self.raw_training_data = np.genfromtxt(raw_training_data_path, delimiter = \",\")\n",
    "            self.raw_validation_data = np.genfromtxt(raw_validation_data_path, delimiter = \",\")\n",
    "            \n",
    "            print(self.raw_training_data[0:10,:])\n",
    "            \n",
    "            # split training features and labels\n",
    "            self.training_features = self.raw_training_data[:, 1:]\n",
    "            self.training_labels = self.raw_training_data[:, 0]\n",
    "            \n",
    "            print(\"No of training samples: {}, No of training features: {}\".format(len(self.training_labels), self.training_features.shape[1]))\n",
    "            \n",
    "            \n",
    "            # split validation features and labels\n",
    "            self.validation_features = self.raw_validation_data[:, 1:]\n",
    "            self.validation_labels = self.raw_validation_data[:, 0]\n",
    "            \n",
    "            print(\"No of validation samples: {}\".format(len(self.validation_labels), ))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree_Node:\n",
    "    \n",
    "    def __init__(self, X = None, y = None, depth = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.depth = depth\n",
    "        self.feature_index_with_optimal_split = None\n",
    "        self.threshold_for_optimal_split = None # since features are real values\n",
    "        self.is_leaf = False\n",
    "        self.class_probabilities = None\n",
    "\n",
    "\n",
    "class Stump_Decision_Tree(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, root = None, max_depth = None, min_pool = None, weights_of_training_samples = None):\n",
    "        '''\n",
    "        Implementation of a stump decision tree i.e., a decision tree with a root and a pair of left and right children.\n",
    "\n",
    "        Parameters\n",
    "        -------------\n",
    "        max_depth : int, optional, default 1\n",
    "                    The maximum depth of the deicison tree. For a stump learner, by default 1.\n",
    "        min_pool :  int, optional, default +INF\n",
    "                    The minimum number of samples at a node after split. For a stump learner, no bound required.\n",
    "        '''\n",
    "        self.root = root\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_pool\n",
    "        self.classes = None\n",
    "        self.weights_of_training_samples = weights_of_training_samples\n",
    "    \n",
    "    \n",
    "    def build_decision_tree(self, X, y, node, current_depth):\n",
    "\n",
    "        '''\n",
    "        Recursively builds the decision tree if the terminal conditions not met\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        X : training features\n",
    "        y : training labels\n",
    "        ''' \n",
    "        \n",
    "        # base cases of recursion\n",
    "        if current_depth >= self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "            \n",
    "        elif len(y) <= self.min_samples:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "        \n",
    "        elif len(np.unique(y)) == 1:\n",
    "            node.is_leaf = True\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # find the best split and the feature that gives this best split\n",
    "            feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right = self.best_split(X, y)#, node.depth) \n",
    "            \n",
    "            \n",
    "            if feature_for_best_split is None:\n",
    "                # best_split() returns None if no feature gave a split that has less impurity than the parent\n",
    "                node.is_leaf = True\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Recursively build trees to the left and the right using the best features and the best split threshold\n",
    "                node.depth = current_depth\n",
    "                node.feature_index_with_optimal_split = feature_for_best_split\n",
    "                node.threshold_for_optimal_split = best_split_threshold\n",
    "                \n",
    "                node.left = Decision_Tree_Node(X_left, y_left, current_depth+1)\n",
    "                node.right = Decision_Tree_Node(X_right, y_right, current_depth+1)\n",
    "                \n",
    "                # when a node is encountered, calculate the class probabilities for each class at this node and store\n",
    "                node.left.class_probabilities = self.calculate_class_probabilities(y_left)\n",
    "                node.right.class_probabilities = self.calculate_class_probabilities(y_right)\n",
    "                \n",
    "                self.build_decision_tree(X_left, y_left, node.left, current_depth+1)\n",
    "                self.build_decision_tree(X_right, y_right, node.right, current_depth+1)\n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "    def weighted_gini_impurity_before_split(self,y):\n",
    "        '''\n",
    "        Calculates the weighted GINI impurity of y before split.\n",
    "        '''\n",
    "        \n",
    "        indices_of_label_3_samples = np.nonzero(y==3)[0]\n",
    "        c_root_pos = 0\n",
    "        \n",
    "        for idx in indices_of_label_3_samples:\n",
    "            c_root_pos += self.weights_of_training_samples[idx]\n",
    "        \n",
    "        \n",
    "        c_root_total = np.sum(self.weights_of_training_samples)  \n",
    "        \n",
    "        \n",
    "        c_root_neg = c_root_total - c_root_pos\n",
    "        gini_of_y = 1 - (c_root_pos / c_root_total) ** 2 - ( c_root_neg/ c_root_total) ** 2\n",
    "        return gini_of_y\n",
    "  \n",
    "\n",
    "    def weighted_gini_impurity_after_split(self, y_copy, indices_where_change, i):\n",
    "        '''\n",
    "        Calculates the weighted GINI impurity metric U after split of y_copy.\n",
    "        \n",
    "        U = (p^2) + (q^2 )\n",
    "        \n",
    "        where p = No of samples at this node with feature x below the node threshold/ No of samples at this node\n",
    "              q = No of samples at this node with feature x equal or above the node threshold/ No of samples at this node\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        '''\n",
    "\n",
    "        indices_of_positive_elements_left = np.nonzero(y_copy[:indices_where_change[i] + 1]== 3)[0]\n",
    "        CL_pos= 0\n",
    "        \n",
    "        for idx in indices_of_positive_elements_left:\n",
    "            CL_pos += self.weights_of_training_samples[idx]\n",
    "            \n",
    "        CL_total = np.sum(self.weights_of_training_samples[:indices_where_change[i] + 1])\n",
    "        CL_neg = CL_total - CL_pos\n",
    "        prob1 = CL_pos / (CL_total)\n",
    "        prob2 = CL_neg / (CL_total)\n",
    "        UAL = 1 - (prob1) ** 2 - (prob2) ** 2\n",
    "        \n",
    "        \n",
    "        \n",
    "        indices_of_positive_elements_right = np.nonzero(y_copy[indices_where_change[i] + 1:]== 3)[0]\n",
    "        CR_pos = 0\n",
    "        for idx in indices_of_positive_elements_right:\n",
    "            CR_pos += self.weights_of_training_samples[idx + indices_where_change[i] + 1]\n",
    "            \n",
    "        CR_total = np.sum(self.weights_of_training_samples[indices_where_change[i] + 1:])\n",
    "        \n",
    "        CR_neg = CR_total - CR_pos\n",
    "        prob1 = CR_pos / (CR_total)\n",
    "        prob2 = CR_neg / (CR_total)\n",
    "        UAR = 1 - (prob1) ** 2 - (prob2) ** 2\n",
    "\n",
    "\n",
    "        pl = (CL_total) / np.sum(self.weights_of_training_samples)\n",
    "        pr = (CR_total) / np.sum(self.weights_of_training_samples)\n",
    "        \n",
    "        weighted_gini_after_split =  pl * UAL + pr * UAR\n",
    "        return weighted_gini_after_split\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def split_node(self, X, y, best_feature, max_benefit_val):\n",
    "        '''\n",
    "        Split a node using the best feature that maximizes Information Gain.\n",
    "        '''\n",
    "        left_X, right_X, left_y, right_y = [],[],[],[]\n",
    "\n",
    "        for row in range(0,X.shape[0]):\n",
    "            if X[row][best_feature] < max_benefit_val:\n",
    "                left_X.append(X[row])\n",
    "                left_y.append(y[row])\n",
    "            else:\n",
    "                right_X.append(X[row])\n",
    "                right_y.append(y[row])\n",
    "        \n",
    "        return np.array(left_X), np.array(right_X), np.array(left_y), np.array(right_y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def best_split(self, X_passed, y_passed):\n",
    "        '''\n",
    "        Divides the training samples into optimal splits according to the impurity metric\n",
    "        '''\n",
    "        numpy_X = []\n",
    "        numpy_y = []\n",
    "        \n",
    "        for row in range(0,len(X_passed)):\n",
    "            numpy_X.append(X_passed[row])\n",
    "            numpy_y.append(y_passed[row])\n",
    "        \n",
    "        X = np.array(numpy_X)\n",
    "        y = np.array(numpy_y)\n",
    "        \n",
    "        \n",
    "        weighted_gini_before_split = self.weighted_gini_impurity_before_split(y)\n",
    "\n",
    "        benefit_of_split = 0\n",
    "        \n",
    "        benefit_vals_for_one_feature = np.zeros(X.shape[1])\n",
    "        max_benefit_index_for_each_feature = np.zeros(X.shape[1])\n",
    "        \n",
    "        \n",
    "        # for each feature, find the GINI gain at each threshold\n",
    "        for feature in range(0,X.shape[1]):\n",
    "            current_feature = X[:, feature]\n",
    "            \n",
    "            # TRICK: Sort the training samples according to this feature's values\n",
    "            # and find GINI only at samples where the class label changes.\n",
    "            \n",
    "            \n",
    "            stacked = np.column_stack((y, current_feature)) # stack y_to_take and current_feature\n",
    "            sortedd = stacked[np.argsort(stacked[:, 1])]\n",
    "            \n",
    "            y_copy = sortedd[..., 0]\n",
    "\n",
    "            indices_where_change = np.where(y_copy[:-1] != y_copy[1:])[0]\n",
    "\n",
    "            # the values in indices_where_change are the indices after which the label changes.\n",
    "            for i in range(len(indices_where_change)):\n",
    "                weighted_gini_after_split = self.weighted_gini_impurity_after_split(y_copy, indices_where_change, i)\n",
    "\n",
    "                benefit_of_split = weighted_gini_before_split - weighted_gini_after_split\n",
    "\n",
    "\n",
    "                if benefit_of_split >  benefit_vals_for_one_feature[feature]:\n",
    "                    benefit_vals_for_one_feature[feature] = benefit_of_split  \n",
    "                    max_benefit_index_for_each_feature[feature] = indices_where_change[i]\n",
    "                                      \n",
    "\n",
    "        if benefit_of_split == 0:\n",
    "            #print(\"No good feature found.\")\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            \n",
    "            best_feature = np.argmax(benefit_vals_for_one_feature)\n",
    "            index_of_threhold_with_max_benefit_for_best_feature = max_benefit_index_for_each_feature[best_feature]\n",
    "            \n",
    "            # lets get the column of that feature and sort and get the threshold\n",
    "            max_benefit_feature = copy.deepcopy(X[:,best_feature])\n",
    "            max_benefit_feature.sort()\n",
    "            \n",
    "            # Athough we have threshold_index_for_best_split_in_indices_where_change, we need to find the\n",
    "            # corresponding index in the sorted max benefit feature.\n",
    "            \n",
    "\n",
    "            max_benefit_val = max_benefit_feature[int(index_of_threhold_with_max_benefit_for_best_feature)+1]\n",
    "            \n",
    "            \n",
    "            X_left, X_right, y_left, y_right = self.split_node(X, y, best_feature, max_benefit_val)\n",
    "            \n",
    "                        \n",
    "            return best_feature, max_benefit_val, X_left, X_right, y_left, y_right\n",
    "                \n",
    "                        \n",
    "\n",
    "                \n",
    "    def calculate_class_probabilities(self, y):\n",
    "        '''\n",
    "        Calculates the probability of each class at this node\n",
    "        '''\n",
    "        \n",
    "        class_probabilities = []\n",
    "        \n",
    "        for label in self.root.classes:\n",
    "            \n",
    "            if len(y[y==label]) == len(y) :\n",
    "                class_prob = 1\n",
    "            elif len(y[y==label]) == 0:\n",
    "                class_prob = 0\n",
    "            else:\n",
    "                class_prob = y[y==label].shape[0]/len(y)\n",
    "            \n",
    "            class_probabilities.append(class_prob)\n",
    "        \n",
    "        return class_probabilities\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    def predict_a_sample(self, x, node):\n",
    "        '''\n",
    "        Predicts a class label for the passed test sample by recursively calling a child node from the root upto\n",
    "        a leaf node of the learned decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        x : a single test sample features\n",
    "        node : a node of the learned decision tree\n",
    "        '''\n",
    "        \n",
    "        if node.is_leaf:\n",
    "            return node.class_probabilities\n",
    "        else:\n",
    "            \n",
    "            if x[node.feature_index_with_optimal_split] < node.threshold_for_optimal_split:\n",
    "                return self.predict_a_sample(x, node.left)\n",
    "            else:\n",
    "                return self.predict_a_sample(x, node.right)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts class labels for all test samples passed.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------------------\n",
    "        X : test samples passed as 2D arrays\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        test_preds = []\n",
    "        for test_sample in X:\n",
    "            class_probabilities = self.predict_a_sample(test_sample, self.root)\n",
    "            test_pred = np.argmax(class_probabilities)\n",
    "            if test_pred == 0:\n",
    "                \n",
    "                test_preds.append(3)\n",
    "            elif test_pred == 1:\n",
    "                test_preds.append(5)\n",
    "            else:\n",
    "                raise AssertionError\n",
    "        \n",
    "        return test_preds\n",
    "        \n",
    "       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Train CART decision tree using GINI impurity.\n",
    "        \n",
    "        Parameters\n",
    "        ----------------\n",
    "        X : arraylike\n",
    "            The training samples in 2D format, corresponding to input features of each sample.\n",
    "        y : arraylike, optional, default None\n",
    "            The labels for each sample in X. Some algorithms do not require the label y while some (like Decisoin Tree) do.\n",
    "            Hence, keep y as optional.\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        self : Trained Decision Tree object\n",
    "               This method returns self for compatibility reasons with sklearn's other interfaces/ functionalities.\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.root = Decision_Tree_Node(X, y, 0)\n",
    "        self.root.classes = list(set(y)) # gives the set of unique elements in y.\n",
    "        self.root.classes.sort() # sorts in ascending order in-place if not already sorted\n",
    "        \n",
    "        #print(\"The unique classes present in training data:{}\".format(self.root.classes))\n",
    "        \n",
    "        self.build_decision_tree(X = X, y = y, node = self.root, current_depth = 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost can be implemented using decision trees in two ways to ensure proper weightage of all the sample.<br>\n",
    "1. Use weighted gini index using sample weights (As in this file) Or,\n",
    "2. For each new weak learner, sample a new data set from the old one proportional to the updated sample weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self, rounds_of_boosting):\n",
    "        \n",
    "        self.train_X = None\n",
    "        self.train_y = None\n",
    "        self.test_X = None\n",
    "        self.text_y = None\n",
    "        \n",
    "        self.sample_weights = []\n",
    "        \n",
    "        self.rounds_of_boosting = rounds_of_boosting\n",
    "        self.random_forest = [] # adaboost maintains a forest of weak learners similar to random forest.\n",
    "        self.weights_of_learners = []\n",
    "    \n",
    "    \n",
    "    def calculate_learner_weight(self, sum_of_weighted_errors):\n",
    "        '''\n",
    "        Calculates alpha parameter that decides the weight of each learner in the final vote.\n",
    "        '''\n",
    "        assert 0 <= sum_of_weighted_errors and sum_of_weighted_errors <= 1\n",
    "        weight_of_learner = 0.5 * ( math.log( (1-sum_of_weighted_errors + 0.00000000000001)/(sum_of_weighted_errors + 0.00000000000001) ) )\n",
    "        \n",
    "        if sum_of_weighted_errors < 0.5:\n",
    "            if not weight_of_learner > 0:\n",
    "                print(\"ValueError \",weight_of_learner)\n",
    "                \n",
    "        \n",
    "        return weight_of_learner\n",
    "    \n",
    "    \n",
    "    def update_sample_weights(self, learner_weight, old_sample_weights, individual_unweighted_errors):\n",
    "        '''\n",
    "        Updates the weights of individual samples based on the error made by the learner.\n",
    "        '''\n",
    "        unnormalized_updated_sample_weights = []\n",
    "        \n",
    "        for sample_weight_idx in range(0,len(old_sample_weights)):\n",
    "            \n",
    "            if individual_unweighted_errors[sample_weight_idx] == 0:\n",
    "                e_power_multiplier = math.exp(-learner_weight)\n",
    "            elif individual_unweighted_errors[sample_weight_idx] == 1:\n",
    "                e_power_multiplier = math.exp(learner_weight)\n",
    "            else:\n",
    "                \n",
    "                raise ValueError\n",
    "            \n",
    "            unnormalized_updated_sample_weights.append(old_sample_weights[sample_weight_idx] * e_power_multiplier)\n",
    "        \n",
    "        total_weight = sum(unnormalized_updated_sample_weights)\n",
    "        normalized_updated_sample_weights = [item/total_weight for item in unnormalized_updated_sample_weights]\n",
    "        \n",
    "        return normalized_updated_sample_weights\n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    def calculate_weighted_error(self, X, y, weights_of_samples, learner):\n",
    "        '''\n",
    "        Calculates the weighted error made on each sample by the learner.\n",
    "        '''        \n",
    "        predictions = learner.predict(X)\n",
    "        individual_unweighted_errors = []\n",
    "        sum_of_weighted_errors = 0\n",
    "        \n",
    "        for sample in range(len(predictions)):\n",
    "            \n",
    "            if y[sample] != predictions[sample]:\n",
    "                sum_of_weighted_errors += weights_of_samples[sample]\n",
    "                individual_unweighted_errors.append(1)\n",
    "            else:\n",
    "                individual_unweighted_errors.append(0)\n",
    "        \n",
    "        \n",
    "        assert 0 <= sum_of_weighted_errors and sum_of_weighted_errors <= 1\n",
    "        assert sum(individual_unweighted_errors) < 0.5 * len(y)\n",
    "        \n",
    "        return sum_of_weighted_errors, individual_unweighted_errors\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts the labels for different samples in X using majority vote of different weak learners.\n",
    "        '''\n",
    "        predictions_from_different_learners = []\n",
    "        \n",
    "        for idx in range(self.rounds_of_boosting):\n",
    "            \n",
    "            # make predictions for all the samples in X using this learner\n",
    "            predictions = self.random_forest[idx].predict(X)\n",
    "            predictions_from_different_learners.append(predictions)\n",
    "            \n",
    "        predictions_from_different_learners = np.array(predictions_from_different_learners)\n",
    "        # Each row contains the predictions for different samples from one learner. \n",
    "        \n",
    "        \n",
    "        predictions_from_different_learners = predictions_from_different_learners.T\n",
    "        # Each row contains the predictions for one sample from different learners. \n",
    "        \n",
    "        # for all samples in X, vote using all the learners\n",
    "        final_predictions = []\n",
    "        \n",
    "        for sample_idx in range(len(X)):\n",
    "            \n",
    "            cumulative_prediction_for_sample = 0\n",
    "            for learner_no in range(self.rounds_of_boosting):\n",
    "                \n",
    "                \n",
    "                \n",
    "                if predictions_from_different_learners[sample_idx][learner_no] == 3:\n",
    "                    cumulative_prediction_for_sample += (self.weights_of_learners[learner_no] )\n",
    "                elif predictions_from_different_learners[sample_idx][learner_no] == 5:\n",
    "                    cumulative_prediction_for_sample -= (self.weights_of_learners[learner_no] )\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            \n",
    "            final_prediction = None\n",
    "            if cumulative_prediction_for_sample >= 0:\n",
    "                final_prediction = 3\n",
    "                \n",
    "            else:\n",
    "                final_prediction = 5\n",
    "                \n",
    "            final_predictions.append(final_prediction)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return final_predictions\n",
    "                \n",
    "                    \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self, X ,y):\n",
    "        '''\n",
    "        Trains an Adaboost instance.\n",
    "        '''\n",
    "        self.train_X = X\n",
    "        self.train_y = y\n",
    "        \n",
    "        leng_of_training_set = len(X)\n",
    "        \n",
    "        current_sample_weights = [1/leng_of_training_set for _ in range(leng_of_training_set)]\n",
    "        self.sample_weights.append(current_sample_weights)\n",
    "        \n",
    "        print(\"Original sample weights: \",current_sample_weights[0:5])\n",
    "        \n",
    "        \n",
    "        for weak_learner_num in range(0,self.rounds_of_boosting):\n",
    "            print(\"\\nWeak Learner: {} is training.\".format(weak_learner_num))\n",
    "            \n",
    "            # train a weak learner\n",
    "            decision_tree_root = Decision_Tree_Node()\n",
    "            weak_learner = Stump_Decision_Tree(root = decision_tree_root, max_depth = 9, min_pool = 1, weights_of_training_samples = current_sample_weights)\n",
    "            weak_learner.fit(X, y)\n",
    "            self.random_forest.append(weak_learner)\n",
    "            \n",
    "            # calculate weighted error\n",
    "            sum_of_weighted_errors, individual_unweighted_errors = self.calculate_weighted_error(self.train_X, self.train_y, current_sample_weights, weak_learner)\n",
    "            \n",
    "            # update alpha for this learner\n",
    "            weight_of_learner = self.calculate_learner_weight(sum_of_weighted_errors)\n",
    "            \n",
    "            self.weights_of_learners.append(weight_of_learner)\n",
    "            \n",
    "            # update the sample weights for all the training samples\n",
    "            current_sample_weights = self.update_sample_weights(weight_of_learner, current_sample_weights, individual_unweighted_errors)\n",
    "            print(\"Updated sample weights: \",current_sample_weights[0:5])\n",
    "            \n",
    "            \n",
    "            # save the updated weights\n",
    "            if weak_learner_num+1 < self.rounds_of_boosting:\n",
    "                self.sample_weights.append(current_sample_weights)\n",
    "            \n",
    "            \n",
    "            \n",
    "        print(\"The voting weights of different learners of Adaboost forest: \",self.weights_of_learners,)    \n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5.     -252.5092 1040.6188 ...  -21.6094  -32.602    25.619 ]\n",
      " [   5.     -684.5502 -368.7191 ...  -36.3467   26.6937  -17.564 ]\n",
      " [   3.      119.2934  638.9885 ...   14.7913   48.7926  -94.5664]\n",
      " ...\n",
      " [   5.      972.0162   77.9232 ...  -35.5166  -16.6162  -43.1298]\n",
      " [   5.     -255.5209  144.6523 ...   75.572    34.6369   24.2973]\n",
      " [   3.      217.1434  592.4619 ...  -25.7318   55.1806    4.9309]]\n",
      "No of training samples: 1000, No of training features: 100\n",
      "No of validation samples: 1629\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "training_data_path = \"data/pa3_train_reduced.csv\"\n",
    "validation_data_path = \"data/pa3_valid_reduced.csv\"\n",
    "\n",
    "preprocessing = Preprocessing(training_data_path, validation_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 1\n",
      "Original sample weights:  [0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "Updated sample weights:  [0.003205128205128148, 0.0005924170616113955, 0.0005924170616113955, 0.0005924170616113955, 0.0005924170616113955]\n",
      "The voting weights of different learners of Adaboost forest:  [0.8441482436731834]\n",
      "Training Accuracy : 0.844 Testing Accuracy : 0.7691835481890731\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 5\n",
      "Original sample weights:  [0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "Updated sample weights:  [0.003205128205128148, 0.0005924170616113955, 0.0005924170616113955, 0.0005924170616113955, 0.0005924170616113955]\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "Updated sample weights:  [0.004467688659269917, 0.0004618882940925846, 0.0004618882940925846, 0.0004618882940925846, 0.0004618882940925846]\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "Updated sample weights:  [0.010305242174679782, 0.0002948603711311317, 0.0002948603711311317, 0.0002948603711311317, 0.0002948603711311317]\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "Updated sample weights:  [0.006598073208601528, 0.00018878841293208881, 0.00018878841293208881, 0.00018878841293208881, 0.00018878841293208881]\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "Updated sample weights:  [0.005425980875723019, 0.00024080610194893997, 0.00024080610194893997, 0.0001552517357328215, 0.0001552517357328215]\n",
      "The voting weights of different learners of Adaboost forest:  [0.8441482436731834, 0.2905034569494873, 0.64230133320498, 0.6355416874456303, 0.21947207621920858]\n",
      "Training Accuracy : 0.937 Testing Accuracy : 0.8281154082259055\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 10\n",
      "Original sample weights:  [0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "Updated sample weights:  [0.003205128205128148, 0.0005924170616113955, 0.0005924170616113955, 0.0005924170616113955, 0.0005924170616113955]\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "Updated sample weights:  [0.004467688659269917, 0.0004618882940925846, 0.0004618882940925846, 0.0004618882940925846, 0.0004618882940925846]\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "Updated sample weights:  [0.010305242174679782, 0.0002948603711311317, 0.0002948603711311317, 0.0002948603711311317, 0.0002948603711311317]\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "Updated sample weights:  [0.006598073208601528, 0.00018878841293208881, 0.00018878841293208881, 0.00018878841293208881, 0.00018878841293208881]\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "Updated sample weights:  [0.005425980875723019, 0.00024080610194893997, 0.00024080610194893997, 0.0001552517357328215, 0.0001552517357328215]\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "Updated sample weights:  [0.005428234821512022, 0.0002407061544262093, 0.00024090613250781693, 0.00015518729788734938, 0.00015518729788734938]\n",
      "\n",
      "Weak Learner: 6 is training.\n",
      "Updated sample weights:  [0.005428234821512269, 0.00024070615442620892, 0.0002409061325078279, 0.00015518729788734914, 0.00015518729788734914]\n",
      "\n",
      "Weak Learner: 7 is training.\n",
      "Updated sample weights:  [0.005428234821512136, 0.0002407061544262142, 0.00024090613250782198, 0.00015518729788735252, 0.00015518729788735252]\n",
      "\n",
      "Weak Learner: 8 is training.\n",
      "Updated sample weights:  [0.005428234821512137, 0.0002407061544262137, 0.00024090613250782203, 0.0001551872978873522, 0.0001551872978873522]\n",
      "\n",
      "Weak Learner: 9 is training.\n",
      "Updated sample weights:  [0.005428234821512142, 0.00024070615442621336, 0.00024090613250782225, 0.000155187297887352, 0.000155187297887352]\n",
      "The voting weights of different learners of Adaboost forest:  [0.8441482436731834, 0.2905034569494873, 0.64230133320498, 0.6355416874456303, 0.21947207621920858, 0.00041522631071895256, 2.3647750424515276e-14, -2.331468351712883e-14, 1.2212453270876706e-15, 1.1102230246251554e-15]\n",
      "Training Accuracy : 0.937 Testing Accuracy : 0.8281154082259055\n",
      "***************************************************\n",
      "\n",
      "No of boosting rounds for current Adaboost: 20\n",
      "Original sample weights:  [0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "\n",
      "Weak Learner: 0 is training.\n",
      "Updated sample weights:  [0.003205128205128148, 0.0005924170616113955, 0.0005924170616113955, 0.0005924170616113955, 0.0005924170616113955]\n",
      "\n",
      "Weak Learner: 1 is training.\n",
      "Updated sample weights:  [0.004467688659269917, 0.0004618882940925846, 0.0004618882940925846, 0.0004618882940925846, 0.0004618882940925846]\n",
      "\n",
      "Weak Learner: 2 is training.\n",
      "Updated sample weights:  [0.010305242174679782, 0.0002948603711311317, 0.0002948603711311317, 0.0002948603711311317, 0.0002948603711311317]\n",
      "\n",
      "Weak Learner: 3 is training.\n",
      "Updated sample weights:  [0.006598073208601528, 0.00018878841293208881, 0.00018878841293208881, 0.00018878841293208881, 0.00018878841293208881]\n",
      "\n",
      "Weak Learner: 4 is training.\n",
      "Updated sample weights:  [0.005425980875723019, 0.00024080610194893997, 0.00024080610194893997, 0.0001552517357328215, 0.0001552517357328215]\n",
      "\n",
      "Weak Learner: 5 is training.\n",
      "Updated sample weights:  [0.005428234821512022, 0.0002407061544262093, 0.00024090613250781693, 0.00015518729788734938, 0.00015518729788734938]\n",
      "\n",
      "Weak Learner: 6 is training.\n",
      "Updated sample weights:  [0.005428234821512269, 0.00024070615442620892, 0.0002409061325078279, 0.00015518729788734914, 0.00015518729788734914]\n",
      "\n",
      "Weak Learner: 7 is training.\n",
      "Updated sample weights:  [0.005428234821512136, 0.0002407061544262142, 0.00024090613250782198, 0.00015518729788735252, 0.00015518729788735252]\n",
      "\n",
      "Weak Learner: 8 is training.\n",
      "Updated sample weights:  [0.005428234821512137, 0.0002407061544262137, 0.00024090613250782203, 0.0001551872978873522, 0.0001551872978873522]\n",
      "\n",
      "Weak Learner: 9 is training.\n",
      "Updated sample weights:  [0.005428234821512142, 0.00024070615442621336, 0.00024090613250782225, 0.000155187297887352, 0.000155187297887352]\n",
      "\n",
      "Weak Learner: 10 is training.\n",
      "Updated sample weights:  [0.005428234821512142, 0.00024070615442621325, 0.00024090613250782225, 0.00015518729788735196, 0.00015518729788735196]\n",
      "\n",
      "Weak Learner: 11 is training.\n",
      "Updated sample weights:  [0.005428234821512143, 0.0002407061544262132, 0.0002409061325078223, 0.00015518729788735193, 0.00015518729788735193]\n",
      "\n",
      "Weak Learner: 12 is training.\n",
      "ValueError  0.0\n",
      "Updated sample weights:  [0.005428234821512143, 0.0002407061544262132, 0.0002409061325078223, 0.00015518729788735193, 0.00015518729788735193]\n",
      "\n",
      "Weak Learner: 13 is training.\n",
      "ValueError  0.0\n",
      "Updated sample weights:  [0.005428234821512143, 0.0002407061544262132, 0.0002409061325078223, 0.00015518729788735193, 0.00015518729788735193]\n",
      "\n",
      "Weak Learner: 14 is training.\n",
      "ValueError  0.0\n",
      "Updated sample weights:  [0.005428234821512143, 0.0002407061544262132, 0.0002409061325078223, 0.00015518729788735193, 0.00015518729788735193]\n",
      "\n",
      "Weak Learner: 15 is training.\n",
      "ValueError  0.0\n",
      "Updated sample weights:  [0.005428234821512143, 0.0002407061544262132, 0.0002409061325078223, 0.00015518729788735193, 0.00015518729788735193]\n",
      "\n",
      "Weak Learner: 16 is training.\n",
      "ValueError  0.0\n",
      "Updated sample weights:  [0.005428234821512143, 0.0002407061544262132, 0.0002409061325078223, 0.00015518729788735193, 0.00015518729788735193]\n",
      "\n",
      "Weak Learner: 17 is training.\n",
      "ValueError  0.0\n",
      "Updated sample weights:  [0.005428234821512143, 0.0002407061544262132, 0.0002409061325078223, 0.00015518729788735193, 0.00015518729788735193]\n",
      "\n",
      "Weak Learner: 18 is training.\n",
      "ValueError  0.0\n",
      "Updated sample weights:  [0.005428234821512143, 0.0002407061544262132, 0.0002409061325078223, 0.00015518729788735193, 0.00015518729788735193]\n",
      "\n",
      "Weak Learner: 19 is training.\n",
      "ValueError  0.0\n",
      "Updated sample weights:  [0.005428234821512143, 0.0002407061544262132, 0.0002409061325078223, 0.00015518729788735193, 0.00015518729788735193]\n",
      "The voting weights of different learners of Adaboost forest:  [0.8441482436731834, 0.2905034569494873, 0.64230133320498, 0.6355416874456303, 0.21947207621920858, 0.00041522631071895256, 2.3647750424515276e-14, -2.331468351712883e-14, 1.2212453270876706e-15, 1.1102230246251554e-15, 2.2204460492503126e-16, 2.2204460492503126e-16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Training Accuracy : 0.937 Testing Accuracy : 0.8281154082259055\n"
     ]
    }
   ],
   "source": [
    "# Observe the trend for different no of trees in the adaboost forest.\n",
    "\n",
    "no_of_boosting_rounds_list = [1,5,10,20]\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for no_of_boosting_rounds in no_of_boosting_rounds_list:\n",
    "    print(\"***************************************************\")\n",
    "    print(\"\\nNo of boosting rounds for current Adaboost: {}\".format(no_of_boosting_rounds))\n",
    "    adaboost_instance = AdaBoost(rounds_of_boosting = no_of_boosting_rounds)\n",
    "    adaboost_instance.fit(preprocessing.training_features, preprocessing.training_labels)\n",
    "    \n",
    "    \n",
    "    # predict for both training data and test data\n",
    "    training_predictions = adaboost_instance.predict(preprocessing.training_features)\n",
    "    training_acc = accuracy_score(preprocessing.training_labels, training_predictions)\n",
    "    training_accuracy.append(training_acc)\n",
    "    \n",
    "    \n",
    "    test_predictions = adaboost_instance.predict(preprocessing.validation_features)\n",
    "    test_acc = accuracy_score(preprocessing.validation_labels, test_predictions)\n",
    "    test_accuracy.append(test_acc)\n",
    "    print(\"Training Accuracy : {} Testing Accuracy : {}\".format(training_acc, test_acc))\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(no_of_boosting_rounds_list, training_accuracy, 'g--', label=\"Training Accuracy\")\n",
    "plt.plot(no_of_boosting_rounds_list, test_accuracy, 'r--', label=\"Test Accuracy\")\n",
    "plt.xlabel(\"No of Trees in Random Forest\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
