{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART Decision Tree for Optical Character Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build a decision tree to classify between two handwritten numbers, 3 and 5. The data set used is from Oregon State University, Fall 2018 CS 534 Machine Learning Course Implementation Assignment 3.<br><br>\n",
    "There are 100 features for each sample, corresponding to the top 100 principal components generated using PCA from the original sample features of size 28 * 28.<br><br>\n",
    "The training set contains 4888 samples. Each sample is a list of 101 values. The first column represents the digitâ€™s label  which is 3 or 5. The other 100 floating values are the PCA-generated features for this sample.<br><br>\n",
    "The validation set contains 1629 samples. This set will be used to tune the hyper parameters and select the best model.<br>\n",
    "No test set is provided.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Decision Tree algorithm does not need any scaling of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, raw_training_data_path, raw_validation_data_path):\n",
    "            \n",
    "            # Read the training and validation data. As the features are just PCA components, no need to observe the \n",
    "            # column names, so can proceed with numpy instead of pandas.\n",
    "            \n",
    "            self.raw_training_data = np.genfromtxt(raw_training_data_path, delimiter = \",\")\n",
    "            self.raw_validation_data = np.genfromtxt(raw_validation_data_path, delimiter = \",\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            # split training features and labels\n",
    "            self.training_features = self.raw_training_data[:, 1:]\n",
    "            self.training_labels = self.raw_training_data[:, 0]\n",
    "            \n",
    "            print(\"No of training samples: {}, No of training features: {}\".format(len(self.training_labels), self.training_features.shape[1]))\n",
    "            \n",
    "            \n",
    "            # split validation features and labels\n",
    "            self.validation_features = self.raw_validation_data[:, 1:]\n",
    "            self.validation_labels = self.raw_validation_data[:, 0]\n",
    "            \n",
    "            print(\"No of validation samples: {}\".format(len(self.training_labels), ))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation with GINI Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree_Node:\n",
    "    \n",
    "    def __init__(self, X = None, y = None, depth = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "#         self.left_X = None\n",
    "#         self.right_X = None\n",
    "#         self.left_Y = None\n",
    "#         self.right_Y = None\n",
    "        self.depth = depth\n",
    "        self.feature_index_with_optimal_split = None\n",
    "        self.threshold_for_optimal_split = None # since features are real values\n",
    "        self.is_leaf = False\n",
    "        self.class_probabilities = None\n",
    "\n",
    "\n",
    "class Decision_Tree(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    '''\n",
    "    Implementation of CART Decision Tree.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    max_depth : int, optional, default 100\n",
    "                The maximum depth of the deicison tree for early-stopping/ to avoid overfitting to outliers.\n",
    "    min_pool :  int, optional, default 10\n",
    "                The minimum number of samples at a node after split for early-stopping/ to avoid overfitting.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, max_depth = 100, min_pool = 10,):\n",
    "        \n",
    "        self.root = Decision_Tree_Node(0)\n",
    "        self.max_depth = 100\n",
    "        self.min_samples = min_pool\n",
    "        self.depth = 0\n",
    "        self.classes = None\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def build_decision_tree(self, X, y, node, current_depth):\n",
    "        '''\n",
    "        Recursively builds the decision tree if the terminal conditions not met\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        X : training features\n",
    "        y : training labels\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # base cases of recursion\n",
    "        if current_depth == self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "            \n",
    "        elif len(X) <= self.min_samples:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "            \n",
    "        elif len(y) < self.min_samples:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # when a node is encountered, calculate the class probabilities for each class at this node and store\n",
    "            self.classes = self.calculate_class_probabilities(y)\n",
    "            \n",
    "            # find the best split and the feature that gives this best split\n",
    "            feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right = self.best_split(X, y)#, node.depth) \n",
    "            \n",
    "            \n",
    "            if feature_for_best_split is None:\n",
    "                # best_split() returns None if no feature gave a split that has less impurity than the parent\n",
    "                node.is_leaf = True\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Recursively build trees to the left and the right using the best features and the best split threshold\n",
    "                self.depth = current_depth\n",
    "                self.feature_index_with_optimal_split = feature_for_best_split\n",
    "                self.threshold_for_optimal_split = best_split_threshold\n",
    "                \n",
    "                \n",
    "                left_node = Decision_Tree_Node(X_left, y_left, current_depth+1)\n",
    "                right_node = Decision_Tree_Node(X_right, y_right, current_depth+1)\n",
    "                \n",
    "                \n",
    "                self.build_decision_tree(X_left, y_left, left_node, current_depth+1)\n",
    "                self.build_decision_tree(X_right, y_right, right_node, current_depth+1)\n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def best_split(self, X, y):\n",
    "        '''\n",
    "        Divides the training samples into optimal splits according to the impurity metric\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        gini_impurity_before_split = self.calculate_gini_impurity(y)\n",
    "        lowest_impurity_till_now = gini_impurity_before_split\n",
    "        feature_for_best_split = None\n",
    "        threshold_for_best_split = None\n",
    "        \n",
    "        # for each feature, find the GINI gain at each threshold\n",
    "        for feature in range(X.shape[1]):\n",
    "            \n",
    "            # make a new copy of the data set features and labels\n",
    "            X_copy = copy.deepcopy(X)\n",
    "            y_copy = copy.deepcopy(y)\n",
    "            \n",
    "            current_feature = X_copy[:, feature]\n",
    "            \n",
    "            # TRICK: Sort the training samples according to this feature's values\n",
    "            # and find GINI only at samples where the class label changes.\n",
    "            \n",
    "            # sort according to the current feature\n",
    "            sorting_indices = X_copy[:,feature].argsort()\n",
    "            X_copy_sorted = X_copy[sorting_indices]\n",
    "            \n",
    "            y_copy_sorted = y_copy[sorting_indices]\n",
    "            \n",
    "            thresholds = []\n",
    "            threshold_impurities = []\n",
    "            best_threhold = None\n",
    "            \n",
    "            \n",
    "            prev_label = None\n",
    "            \n",
    "            for sample_num in range(len(X_copy)):\n",
    "                \n",
    "                if prev_label is None:\n",
    "                    prev_label = y_copy_sorted[sample_num]\n",
    "                \n",
    "                if prev_label is not None: # this means that this is not the first sample\n",
    "                    \n",
    "                    # if the class label changed, set this feature value as the threshold\n",
    "                    # make sure you do not split if the sample number is less\n",
    "                    if y_copy_sorted[sample_num] != prev_label and sample_num >= self.min_samples and len(y)-sample_num >= self.min_samples:\n",
    "                        \n",
    "                        threshold = X_copy_sorted[sample_num,feature]\n",
    "                        thresholds.append(threshold)\n",
    "                        \n",
    "                        \n",
    "                        # all the indices below sample num belong to the left child\n",
    "                        \n",
    "                        # calculate gini impurities\n",
    "                        below_threshold_impurity = self.calculate_impurity(y_copy_sorted[0:sample_num])\n",
    "                        above_threshold_impurity = self.calculate_impurity(y_copy_sorted[sample_num:])\n",
    "                        \n",
    "                        \n",
    "                        # calculate gain\n",
    "                        prob1 = sample_num+1/len(y)\n",
    "                        prob2 = (len(y) - (sample_num+1) )/len(y)\n",
    "                        \n",
    "                        threhold_impurity = (prob1 * below_threshold_impurity + prob2 * above_threshold_impurity)\n",
    "                        threshold_impurities.append(threshold_impurity)\n",
    "                        \n",
    "                        \n",
    "                        if threhold_impurity < lowest_impurity_till_now:\n",
    "                            lowest_impurity_till_now = threhold_impurity\n",
    "                            feature_for_best_split = feature\n",
    "                            threshold_for_best_split = threshold\n",
    "                            \n",
    "                            \n",
    "                        \n",
    "                        # update prev_label\n",
    "                        prev_label = y_copy_sorted[sample_num]\n",
    "                        \n",
    "                \n",
    "                    \n",
    "        # if no feature gave a split that has less impurity than the parent, return None indicating this is the leaf node.\n",
    "        if lowest_impurity == gini_impurity_before_split:\n",
    "            return None\n",
    "        else:\n",
    "            \n",
    "            # make the best split and return\n",
    "            # notice the use of the original X and y passes, instead of the deepcopy created\n",
    "            left_indices = X[:, best_feature] < best_split_threshold\n",
    "            right_indices = X[:, best_feature] >= best_split_threshold\n",
    "            \n",
    "            x_left = X[left_indices]\n",
    "            x_right = X[right_indices]\n",
    "            \n",
    "            y_left = y[left_indices]\n",
    "            y_right = y[right_indices]\n",
    "            \n",
    "            return feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right\n",
    "                \n",
    "                        \n",
    "\n",
    "    def calculate_class_probabilities(self, y):\n",
    "        '''\n",
    "        Calculates the probability of each class at this node\n",
    "        '''\n",
    "        \n",
    "        class_probabilities = []\n",
    "        \n",
    "        for label in self.classes:\n",
    "            class_prob = y[y==label].shape[0]/len(y)\n",
    "            class_probabilities.append(class_prob)\n",
    "        \n",
    "        return class_probabilities\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def calculate_gini_impurity(self, labels):\n",
    "        '''\n",
    "        Calculate GINI impurity metric U at a node of the decision tree\n",
    "        \n",
    "        U = 1 - [(p^2) + (q^2 )]\n",
    "        \n",
    "        where p = No of samples at this node with feature x below the node threshold/ No of samples at this node\n",
    "              q = No of samples at this node with feature x equal or above the node threshold/ No of samples at this node\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        probabilities = []\n",
    "        \n",
    "        for class_label in self.classes:\n",
    "            probability_of_class = labels[labels == class_label]/ len(labels)\n",
    "            probabilities.append(probability_of_class)\n",
    "        \n",
    "        impurity = 1 + sum([elem^2 for elem in probabilities])\n",
    "        \n",
    "        assert impurity <= 1\n",
    "        \n",
    "        return impurity\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    def predict_a_sample(self, x, node):\n",
    "        '''\n",
    "        Predicts a class label for the passed test sample by recursively calling a child node from the root upto\n",
    "        a leaf node of the learned decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        x : a single test sample features\n",
    "        node : a node of the learned decision tree\n",
    "        '''\n",
    "        \n",
    "        if node.is_terminal:\n",
    "            return node.class_probabilities\n",
    "        else:\n",
    "            if x[node.feature_index_with_optimal_split] < node.threshold:\n",
    "                # return class probabilities of the left child\n",
    "                return self.predict(x, node.left)\n",
    "            else:\n",
    "                return self.predict(x, node.right)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts class labels for all test samples passed.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------------------\n",
    "        X : test samples passed as 2D arrays\n",
    "        '''\n",
    "        \n",
    "        test_preds = []\n",
    "        for test_sample in X:\n",
    "            class_probabilities = self.predict_a_sample(test_sample, self.root)\n",
    "            test_pred = np.argmax(class_probabilities)\n",
    "            test_preds.append(test_pred)\n",
    "        \n",
    "        return test_preds\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        '''\n",
    "        Train CART decision tree using GINI impurity.\n",
    "        \n",
    "        Parameters\n",
    "        ----------------\n",
    "        X : arraylike\n",
    "            The training samples in 2D format, corresponding to input features of each sample.\n",
    "        y : arraylike, optional, default None\n",
    "            The labels for each sample in X. Some algorithms do not require the label y while some (like Decisoin Tree) do.\n",
    "            Hence, keep y as optional.\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        self : Trained Decision Tree object\n",
    "               This method returns self for compatibility reasons with sklearn's other interfaces/ functionalities.\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.root_node = Decision_Tree_Node(X, y, 0)\n",
    "        self.classes = list(set(y)) # gives the set of unique elements in y.\n",
    "        self.classes.sort() # sorts in ascending order in-place if not already sorted\n",
    "        self.build_decision_tree(X, y, self.root_node, 0)\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training samples: 4888, No of training features: 100\n",
      "No of validation samples: 4888\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "training_data_path = \"data/pa3_train_reduced.csv\"\n",
    "validation_data_path = \"data/pa3_train_reduced.csv\"\n",
    "\n",
    "preprocessing = Preprocessing(training_data_path, validation_data_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Analysis\n",
    "We can try and see if there exists just one feature that has good threholds that separates different classes in the pairplots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in range(1,len(preprocessing.training_features[0])):\n",
    "    feature_and_target = np.stack((  preprocessing.training_features[:,0], preprocessing.training_features[:,col] ), axis = 1)\n",
    "    #sns.pairplot(pd.DataFrame(feature_and_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-6f63699e2cc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdecision_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecision_Tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdecision_tree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-d8fc66bdc695>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# gives the set of unique elements in y.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sorts in ascending order in-place if not already sorted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_decision_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-d8fc66bdc695>\u001b[0m in \u001b[0;36mbuild_decision_tree\u001b[1;34m(self, X, y, node, current_depth)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;31m# find the best split and the feature that gives this best split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mfeature_for_best_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_split_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_right\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_right\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, node.depth)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-d8fc66bdc695>\u001b[0m in \u001b[0;36mbest_split\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mgini_impurity_before_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_gini_impurity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[0mlowest_impurity_till_now\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgini_impurity_before_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mfeature_for_best_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-d8fc66bdc695>\u001b[0m in \u001b[0;36mcalculate_gini_impurity\u001b[1;34m(self, labels)\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mprobabilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobability_of_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[0mimpurity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m^\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mimpurity\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-d8fc66bdc695>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mprobabilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobability_of_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[0mimpurity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m^\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mimpurity\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# train\n",
    "decision_tree = Decision_Tree(max_depth = 100, min_pool = 10)\n",
    "decision_tree.fit(preprocessing.training_features, preprocessing.training_labels)\n",
    "\n",
    "# predict\n",
    "test_predictions = decision_tree.predict(preprocessing.validation_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
