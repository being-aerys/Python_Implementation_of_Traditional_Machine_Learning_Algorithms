{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART Decision Tree for Optical Character Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build a decision tree to classify between two handwritten numbers, 3 and 5. The data set used is from Oregon State University, Fall 2018 CS 534 Machine Learning Course Implementation Assignment 3.<br><br>\n",
    "There are 100 features for each sample, corresponding to the top 100 principal components generated using PCA from the original sample features of size 28 * 28.<br><br>\n",
    "The training set contains 4888 samples. Each sample is a list of 101 values. The first column represents the digitâ€™s label  which is 3 or 5. The other 100 floating values are the PCA-generated features for this sample.<br><br>\n",
    "The validation set contains 1629 samples. This set will be used to tune the hyper parameters and select the best model.<br>\n",
    "No test set is provided.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Decision Tree algorithm does not need any scaling of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, raw_training_data_path, raw_validation_data_path):\n",
    "            \n",
    "            # Read the training and validation data. As the features are just PCA components, no need to observe the \n",
    "            # column names, so can proceed with numpy instead of pandas.\n",
    "            \n",
    "            self.raw_training_data = np.genfromtxt(raw_training_data_path, delimiter = \",\")\n",
    "            self.raw_validation_data = np.genfromtxt(raw_validation_data_path, delimiter = \",\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            # split training features and labels\n",
    "            self.training_features = self.raw_training_data[:, 0:-1]\n",
    "            self.training_labels = self.raw_training_data[:, -1]\n",
    "            \n",
    "            print(\"No of training samples: {}, No of training features: {}\".format(len(self.training_labels), self.training_features.shape[1]))\n",
    "            \n",
    "            \n",
    "            # split validation features and labels\n",
    "            self.validation_features = self.raw_validation_data[:, 0:-1]\n",
    "            self.validation_labels = self.raw_validation_data[:, -1]\n",
    "            \n",
    "            print(\"No of validation samples: {}\".format(len(self.training_labels), ))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation with GINI Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree_Node:\n",
    "    \n",
    "    def __init__(self, X, y, depth):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.left_X = None\n",
    "        self.right_X = None\n",
    "        self.left_Y = None\n",
    "        self.right_Y = None\n",
    "        self.depth = depth\n",
    "        self.feature_index_with_optimal_split = None\n",
    "        self.threshold_for_optimal_split = None # since features are real values\n",
    "        self.is_leaf = False\n",
    "\n",
    "\n",
    "class Deicision_Tree(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    '''\n",
    "    Implementation of CART Decision Tree.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    max_depth : int, optional, default 100\n",
    "                The maximum depth of the deicison tree for early-stopping/ to avoid overfitting to outliers.\n",
    "    min_pool :  int, optional, default 10\n",
    "                The minimum number of samples at a node after split for early-stopping/ to avoid overfitting.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, max_depth = 100, min_pool = 10,):\n",
    "        \n",
    "        self.root = Decision_Tree_Node(0)\n",
    "        self.max_depth = 100\n",
    "        self.min_samples = 10\n",
    "        self.depth = 0\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def build_decision_tree(self, X, y, node, current_depth):\n",
    "        '''\n",
    "        Recursively builds the decision tree if the terminal conditions not met\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        X : training features\n",
    "        y : training labels\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # base cases of recursion\n",
    "        if current_depth == self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "            \n",
    "        elif len(training_features) <= self.min_pool:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "            \n",
    "        elif len(y) < self.min_pool:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "        else:\n",
    "            # find the best split and the feature that gives this best split\n",
    "            feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right = self.best_split(node.X, node.y, depth) \n",
    "            \n",
    "            \n",
    "            if feature_for_best_split is None:\n",
    "                node.is_leaf = True\n",
    "            else:\n",
    "                \n",
    "                # Recursively build trees to the left and the right using the best features and the best split threshold\n",
    "                self.depth = current_depth\n",
    "                self.feature_index_with_optimal_split = feature_for_best_split\n",
    "                self.threshold_for_optimal_split = best_split_threshold\n",
    "                \n",
    "                \n",
    "                left_node = Decision_Tree_Node(X_left, y_left, current_depth+1)\n",
    "                rightt_node = Decision_Tree_Node(X_right, y_right, current_depth+1)\n",
    "                \n",
    "                \n",
    "                self.build_decision_tree(X_left, y_left, left_node, current_depth+1)\n",
    "                self.build_decision_tree(X_right, y_right, right_node, current_depth+1)\n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def best_split(self, X, y):\n",
    "        '''\n",
    "        Divides the training samples into optimal splits according to the impurity metric\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        gini_impurity_before_split = self.calculate_gini_impurity(y)\n",
    "        lowest_impurity_till_now = gini_impurity_before_split\n",
    "        feature_for_best_split = None\n",
    "        threshold_for_best_split = None\n",
    "        \n",
    "        # for each feature, find the GINI gain at each threshold\n",
    "        for feature in range(X.shape[1]):\n",
    "            \n",
    "            # make a new copy of the data set features and labels\n",
    "            X_copy = copy.deepcopy(X)\n",
    "            y_copy = copy.deepcopy(y)\n",
    "            \n",
    "            current_feature = X_copy[:, feature]\n",
    "            \n",
    "            # TRICK: Sort the training samples according to this feature's values\n",
    "            # and find GINI only at samples where the class label changes.\n",
    "            \n",
    "            # sort according to the current feature\n",
    "            sorting_indices = X_copy[:,feature].argsort()\n",
    "            X_copy_sorted = X_copy[sorting_indices]\n",
    "            \n",
    "            y_copy_sorted = y_copy[sorting_indices]\n",
    "            \n",
    "            thresholds = []\n",
    "            threshold_impurities = []\n",
    "            best_threhold = None\n",
    "            \n",
    "            \n",
    "            prev_label = None\n",
    "            \n",
    "            for sample_num in range(len(X_copy)):\n",
    "                \n",
    "                if prev_label is None:\n",
    "                    prev_label = y_copy_sorted[sample_num]\n",
    "                \n",
    "                if prev_label is not None: # this means that this is not the first sample\n",
    "                    \n",
    "                    # if the class label changed, set this feature value as the threshold\n",
    "                    # make sure you do not split if the sample number is less\n",
    "                    if y_copy_sorted[sample_num] != prev_label and sample_num >= self.min_samples and len(y)-sample_num >= self.min_samples:\n",
    "                        \n",
    "                        threshold = X_copy_sorted[sample_num,feature]\n",
    "                        thresholds.append(threshold)\n",
    "                        \n",
    "                        \n",
    "                        # all the indices below sample num belong to the left child\n",
    "                        \n",
    "                        # calculate gini impurities\n",
    "                        below_threshold_impurity = self.calculate_impurity(y_copy_sorted[0:sample_num])\n",
    "                        above_threshold_impurity = self.calculate_impurity(y_copy_sorted[sample_num:])\n",
    "                        \n",
    "                        \n",
    "                        # calculate gain\n",
    "                        prob1 = sample_num+1/len(y)\n",
    "                        prob2 = (len(y) - (sample_num+1) )/len(y)\n",
    "                        \n",
    "                        threhold_impurity = (prob1 * below_threshold_impurity + prob2 * above_threshold_impurity)\n",
    "                        threshold_impurities.append(threshold_impurity)\n",
    "                        \n",
    "                        \n",
    "                        if threhold_impurity < lowest_impurity_till_now:\n",
    "                            lowest_impurity_till_now = threhold_impurity\n",
    "                            feature_for_best_split = feature\n",
    "                            threshold_for_best_split = threshold\n",
    "                            \n",
    "                            \n",
    "                        \n",
    "                        # update prev_label\n",
    "                        prev_label = y_copy_sorted[sample_num]\n",
    "                        \n",
    "                \n",
    "                    \n",
    "        # if no threshold gave a split thats better than a split previously found in another feature's split\n",
    "        if lowest_impurity == gini_impurity_before_split:\n",
    "            return None\n",
    "        else:\n",
    "            \n",
    "            # make the best split and return\n",
    "            \n",
    "            left_indices = X[:, best_feature] < best_split_threshold\n",
    "            right_indices = X[:, best_feature] >= best_split_threshold\n",
    "            \n",
    "            x_left = X[left_indices]\n",
    "            x_right = X[right_indices]\n",
    "            \n",
    "            y_left = y[left_indices]\n",
    "            y_right = y[right_indices]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            return feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right\n",
    "                \n",
    "                        \n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def split(self, X, y):\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def calculate_gini_impurity(self, labels):\n",
    "        '''\n",
    "        Calculate GINI impurity metric U at a node of the decision tree\n",
    "        \n",
    "        U = 1 - [(p^2) + (q^2 )]\n",
    "        \n",
    "        where p = No of samples at this node with feature x below the node threshold/ No of samples at this node\n",
    "              q = No of samples at this node with feature x equal or above the node threshold/ No of samples at this node\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        probabilities = []\n",
    "        \n",
    "        for class_label in self.unique_labels:\n",
    "            probability_of_class = labels[labels == class_label]/ len(labels)\n",
    "            probabilities.append(probability_of_class)\n",
    "        \n",
    "        impurity = 1 + sum([elem^2 for elem in probabilities])\n",
    "        \n",
    "        assert impurity <= 1\n",
    "        \n",
    "        return impurity\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        '''\n",
    "        Train CART decision tree using GINI impurity.\n",
    "        \n",
    "        Parameters\n",
    "        ----------------\n",
    "        X : arraylike\n",
    "            The training samples in 2D format, corresponding to input features of each sample.\n",
    "        y : arraylike, optional, default None\n",
    "            The labels for each sample in X. Some algorithms do not require the label y while some (like Decisoin Tree) do.\n",
    "            Hence, keep y as optional.\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        self : Trained Decision Tree object\n",
    "               This method returns self for compatibility reasons with sklearn's other interfaces/ functionalities.\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.root_node = Decision_Tree_Node(X, y, 0)\n",
    "        self.build_decision_tree(X, y, self.root_node, 0)\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
