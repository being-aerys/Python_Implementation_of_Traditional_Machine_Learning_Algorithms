{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART Decision Tree for Optical Character Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build a decision tree to classify between two handwritten numbers, 3 and 5. The data set used is from Oregon State University, Fall 2018 CS 534 Machine Learning Course Implementation Assignment 3.<br><br>\n",
    "There are 100 features for each sample, corresponding to the top 100 principal components generated using PCA from the original sample features of size 28 * 28.<br><br>\n",
    "The training set contains 4888 samples. Each sample is a list of 101 values. The first column represents the digitâ€™s label  which is 3 or 5. The other 100 floating values are the PCA-generated features for this sample.<br><br>\n",
    "The validation set contains 1629 samples. This set will be used to tune the hyper parameters and select the best model.<br>\n",
    "No test set is provided.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Decision Tree algorithm does not need any scaling of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, raw_training_data_path, raw_validation_data_path):\n",
    "            \n",
    "            # Read the training and validation data. As the features are just PCA components, no need to observe the \n",
    "            # column names, so can proceed with numpy instead of pandas.\n",
    "            \n",
    "            self.raw_training_data = np.genfromtxt(raw_training_data_path, delimiter = \",\")\n",
    "            self.raw_validation_data = np.genfromtxt(raw_validation_data_path, delimiter = \",\")\n",
    "            \n",
    "            print(self.raw_training_data[0:10,:])\n",
    "            \n",
    "            # split training features and labels\n",
    "            self.training_features = self.raw_training_data[:, 1:]\n",
    "            self.training_labels = self.raw_training_data[:, 0]\n",
    "            \n",
    "            print(\"No of training samples: {}, No of training features: {}\".format(len(self.training_labels), self.training_features.shape[1]))\n",
    "            \n",
    "            \n",
    "            # split validation features and labels\n",
    "            self.validation_features = self.raw_validation_data[:, 1:]\n",
    "            self.validation_labels = self.raw_validation_data[:, 0]\n",
    "            \n",
    "            print(\"No of validation samples: {}\".format(len(self.validation_labels), ))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation with GINI Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree_Node:\n",
    "    \n",
    "    def __init__(self, X = None, y = None, depth = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.depth = depth\n",
    "        self.feature_index_with_optimal_split = None\n",
    "        self.threshold_for_optimal_split = None # since features are real values\n",
    "        self.is_leaf = False\n",
    "        self.class_probabilities = None\n",
    "\n",
    "\n",
    "class Decision_Tree(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    '''\n",
    "    Implementation of CART Decision Tree.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    max_depth : int, optional, default 100\n",
    "                The maximum depth of the deicison tree for early-stopping/ to avoid overfitting to outliers.\n",
    "    min_pool :  int, optional, default 10\n",
    "                The minimum number of samples at a node after split for early-stopping/ to avoid overfitting.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, root, max_depth = None, min_pool = None,):\n",
    "        \n",
    "        self.root = root\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_pool\n",
    "        self.classes = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def build_decision_tree(self, X, y, node, current_depth):\n",
    "\n",
    "        '''\n",
    "        Recursively builds the decision tree if the terminal conditions not met\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        X : training features\n",
    "        y : training labels\n",
    "        ''' \n",
    "        \n",
    "        # base cases of recursion\n",
    "        if current_depth >= self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "            \n",
    "        elif len(y) <= self.min_samples:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "        \n",
    "        elif len(np.unique(y)) == 1:\n",
    "            node.is_leaf = True\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            \n",
    "            \n",
    "            # find the best split and the feature that gives this best split\n",
    "            feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right = self.best_split(X, y)#, node.depth) \n",
    "            \n",
    "            \n",
    "            if feature_for_best_split is None:\n",
    "                # best_split() returns None if no feature gave a split that has less impurity than the parent\n",
    "                node.is_leaf = True\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Recursively build trees to the left and the right using the best features and the best split threshold\n",
    "                node.depth = current_depth\n",
    "                node.feature_index_with_optimal_split = feature_for_best_split\n",
    "                node.threshold_for_optimal_split = best_split_threshold\n",
    "                \n",
    "                node.left = Decision_Tree_Node(X_left, y_left, current_depth+1)\n",
    "                node.right = Decision_Tree_Node(X_right, y_right, current_depth+1)\n",
    "                \n",
    "                # when a node is encountered, calculate the class probabilities for each class at this node and store\n",
    "                node.left.class_probabilities = self.calculate_class_probabilities(y_left)\n",
    "                node.right.class_probabilities = self.calculate_class_probabilities(y_right)\n",
    "                \n",
    "                self.build_decision_tree(X_left, y_left, node.left, current_depth+1)\n",
    "                self.build_decision_tree(X_right, y_right, node.right, current_depth+1)\n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "    def gini_data(self,y): #####################MAKE THIS EFFICEINT\n",
    "        '''\n",
    "        Calculates the GINI impurity of y before split.\n",
    "        '''\n",
    "        #count the Y equal to one\n",
    "        c_root_pos=np.count_nonzero(y==3) # The labels are 3 and 5, 3 is the first label in ascending order.\n",
    "        c_root_total = len(y)        \n",
    "        \n",
    "        gini_of_y =  1- (c_root_pos/c_root_total)**2 -((c_root_total-c_root_pos)/c_root_total)**2 \n",
    "        return gini_of_y\n",
    "  \n",
    "\n",
    "    def gini_after_split(self, y_copy, indices_where_change, i):\n",
    "        \n",
    "        '''\n",
    "        Calculates GINI impurity metric U after split of y_copy.\n",
    "        \n",
    "        U = (p^2) + (q^2 )\n",
    "        \n",
    "        where p = No of samples at this node with feature x below the node threshold/ No of samples at this node\n",
    "              q = No of samples at this node with feature x equal or above the node threshold/ No of samples at this node\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        '''\n",
    "        \n",
    "        CL_pos = np.count_nonzero(y_copy[:indices_where_change[i] + 1] == 3)\n",
    "        CL_total = len(y_copy[:indices_where_change[i] + 1])\n",
    "        CL_neg = CL_total - CL_pos\n",
    "        prob1 = CL_pos / (CL_total)\n",
    "        prob2 = CL_neg / (CL_total)\n",
    "        UAL = 1 - (prob1) ** 2 - (prob2) ** 2\n",
    "\n",
    "\n",
    "\n",
    "        CR_pos = np.count_nonzero(y_copy[indices_where_change[i] + 1:] == 3)\n",
    "        CR_total = len(y_copy[indices_where_change[i] + 1:])\n",
    "        CR_neg = CR_total - CR_pos\n",
    "        prob1 = CR_pos / (CR_total)\n",
    "        prob2 = CR_neg / (CR_total)\n",
    "        UAR = 1 - (prob1) ** 2 - (prob2) ** 2\n",
    "\n",
    "\n",
    "        pl = (CL_total) / len(y_copy)\n",
    "        pr = (CR_total) / len(y_copy)\n",
    "        \n",
    "        gini_after_split =  pl * UAL + pr * UAR\n",
    "        \n",
    "        return gini_after_split\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def split_node(self, X, y, best_feature, max_benefit_val):\n",
    "        '''\n",
    "        Split a node using the best feature that maximizes Information Gain.\n",
    "        '''\n",
    "        left_X, right_X, left_y, right_y = [],[],[],[]\n",
    "\n",
    "        for row in range(0,X.shape[0]):\n",
    "            if X[row][best_feature] < max_benefit_val:\n",
    "                left_X.append(X[row])\n",
    "                left_y.append(y[row])\n",
    "            else:\n",
    "                right_X.append(X[row])\n",
    "                right_y.append(y[row])\n",
    "        \n",
    "        return np.array(left_X), np.array(right_X), np.array(left_y), np.array(right_y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def best_split(self, X_passed, y_passed):\n",
    "        '''\n",
    "        Divides the training samples into optimal splits according to the impurity metric\n",
    "        '''\n",
    "        numpy_X = []\n",
    "        numpy_y = []\n",
    "        \n",
    "        for row in range(0,len(X_passed)):\n",
    "            numpy_X.append(X_passed[row])\n",
    "            numpy_y.append(y_passed[row])\n",
    "        \n",
    "        X = np.array(numpy_X)\n",
    "        y = np.array(numpy_y)\n",
    "        \n",
    "        \n",
    "        gini_before_split = self.gini_data(y)\n",
    "\n",
    "        benefit_of_split = 0\n",
    "        \n",
    "        benefit_vals_for_one_feature = np.zeros(X.shape[1])\n",
    "        max_benefit_index_for_each_feature = np.zeros(X.shape[1])\n",
    "\n",
    "        \n",
    "        # for each feature, find the GINI gain at each threshold\n",
    "        for feature in range(0,X.shape[1]):\n",
    "            \n",
    "            current_feature = X[:, feature]\n",
    "            \n",
    "            # TRICK: Sort the training samples according to this feature's values\n",
    "            # and find GINI only at samples where the class label changes.\n",
    "            \n",
    "            \n",
    "            stacked = np.column_stack((y, current_feature)) # stack y_to_take and current_feature\n",
    "            sortedd = stacked[np.argsort(stacked[:, 1])]\n",
    "            \n",
    "            y_copy = sortedd[..., 0]\n",
    "            #indices_where_change = np.where(np.roll(y_copy,1)!=y_copy)[0]\n",
    "\n",
    "            indices_where_change = np.where(y_copy[:-1] != y_copy[1:])[0]\n",
    "            \n",
    "            # the values in indices_where_change are the indices after which the label changes.\n",
    "                                            \n",
    "            for i in range(len(indices_where_change)):\n",
    "\n",
    "                gini_after_split = self.gini_after_split(y_copy, indices_where_change, i)\n",
    "\n",
    "                benefit_of_split = gini_before_split - gini_after_split\n",
    "\n",
    "\n",
    "                if benefit_of_split >  benefit_vals_for_one_feature[feature]:\n",
    "                    benefit_vals_for_one_feature[feature] = benefit_of_split  \n",
    "                    max_benefit_index_for_each_feature[feature] = indices_where_change[i]\n",
    "                   \n",
    "                \n",
    "                    \n",
    "\n",
    "        if benefit_of_split == 0:\n",
    "            #print(\"No good feature found.\")\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            \n",
    "            best_feature = np.argmax(benefit_vals_for_one_feature)\n",
    "            index_of_threhold_with_max_benefit_for_best_feature = max_benefit_index_for_each_feature[best_feature]\n",
    "            \n",
    "            # lets get the column of that feature and sort and get the threshold\n",
    "            max_benefit_feature = copy.deepcopy(X[:,best_feature])\n",
    "            max_benefit_feature.sort()\n",
    "            \n",
    "            # Athough we have threshold_index_for_best_split_in_indices_where_change, we need to find the\n",
    "            # corresponding index in the sorted max benefit feature.\n",
    "            \n",
    "\n",
    "            max_benefit_val = max_benefit_feature[int(index_of_threhold_with_max_benefit_for_best_feature)+1]\n",
    "            \n",
    "            \n",
    "            X_left, X_right, y_left, y_right = self.split_node(X, y, best_feature, max_benefit_val)\n",
    "            \n",
    "                        \n",
    "            # feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right\n",
    "            return best_feature, max_benefit_val, X_left, X_right, y_left, y_right\n",
    "                \n",
    "                        \n",
    "\n",
    "                \n",
    "    def calculate_class_probabilities(self, y):\n",
    "        '''\n",
    "        Calculates the probability of each class at this node\n",
    "        '''\n",
    "        \n",
    "        class_probabilities = []\n",
    "        \n",
    "        for label in self.root.classes:\n",
    "            \n",
    "            if len(y[y==label]) == len(y) :\n",
    "                class_prob = 1\n",
    "            elif len(y[y==label]) == 0:\n",
    "                class_prob = 0\n",
    "            else:\n",
    "                class_prob = y[y==label].shape[0]/len(y)\n",
    "            \n",
    "            class_probabilities.append(class_prob)\n",
    "        \n",
    "        return class_probabilities\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    def predict_a_sample(self, x, node):\n",
    "        '''\n",
    "        Predicts a class label for the passed test sample by recursively calling a child node from the root upto\n",
    "        a leaf node of the learned decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        x : a single test sample features\n",
    "        node : a node of the learned decision tree\n",
    "        '''\n",
    "        \n",
    "        if node.is_leaf:\n",
    "            return node.class_probabilities\n",
    "        else:\n",
    "            \n",
    "            if x[node.feature_index_with_optimal_split] < node.threshold_for_optimal_split:\n",
    "                return self.predict_a_sample(x, node.left)\n",
    "            else:\n",
    "                return self.predict_a_sample(x, node.right)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts class labels for all test samples passed.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------------------\n",
    "        X : test samples passed as 2D arrays\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        test_preds = []\n",
    "        for test_sample in X:\n",
    "            class_probabilities = self.predict_a_sample(test_sample, self.root)\n",
    "            test_pred = np.argmax(class_probabilities)\n",
    "            if test_pred == 0:\n",
    "                \n",
    "                test_preds.append(3)\n",
    "            elif test_pred == 1:\n",
    "                test_preds.append(5)\n",
    "            else:\n",
    "                raise AssertionError\n",
    "        \n",
    "        return test_preds\n",
    "        \n",
    "       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        '''\n",
    "        Train CART decision tree using GINI impurity.\n",
    "        \n",
    "        Parameters\n",
    "        ----------------\n",
    "        X : arraylike\n",
    "            The training samples in 2D format, corresponding to input features of each sample.\n",
    "        y : arraylike, optional, default None\n",
    "            The labels for each sample in X. Some algorithms do not require the label y while some (like Decisoin Tree) do.\n",
    "            Hence, keep y as optional.\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        self : Trained Decision Tree object\n",
    "               This method returns self for compatibility reasons with sklearn's other interfaces/ functionalities.\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.root = Decision_Tree_Node(X, y, 0)\n",
    "        self.root.classes = list(set(y)) # gives the set of unique elements in y.\n",
    "        self.root.classes.sort() # sorts in ascending order in-place if not already sorted\n",
    "        \n",
    "        print(\"The unique classes present in training data:{}\".format(self.root.classes))\n",
    "        \n",
    "        self.build_decision_tree(X = X, y = y, node = self.root, current_depth = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5.     -252.5092 1040.6188 ...  -21.6094  -32.602    25.619 ]\n",
      " [   5.     -684.5502 -368.7191 ...  -36.3467   26.6937  -17.564 ]\n",
      " [   3.      119.2934  638.9885 ...   14.7913   48.7926  -94.5664]\n",
      " ...\n",
      " [   5.      972.0162   77.9232 ...  -35.5166  -16.6162  -43.1298]\n",
      " [   5.     -255.5209  144.6523 ...   75.572    34.6369   24.2973]\n",
      " [   3.      217.1434  592.4619 ...  -25.7318   55.1806    4.9309]]\n",
      "No of training samples: 4888, No of training features: 100\n",
      "No of validation samples: 1629\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "training_data_path = \"data/pa3_train_reduced.csv\"\n",
    "validation_data_path = \"data/pa3_valid_reduced.csv\"\n",
    "\n",
    "preprocessing = Preprocessing(training_data_path, validation_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Analysis\n",
    "We can try and see if there exists just one feature that has good threholds that roughly separates different classes in the pairplots. (Only an analytical tool, can be skipped here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in range(1,len(preprocessing.training_features[0])):\n",
    "    feature_and_target = np.stack((  preprocessing.training_features[:,0], preprocessing.training_features[:,col] ), axis = 1)\n",
    "    #sns.pairplot(pd.DataFrame(feature_and_target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN\n",
    "Train for a single max_depth and single min_pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique classes present in training data:[3.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "root = Decision_Tree_Node(0)\n",
    "decision_tree = Decision_Tree(root, max_depth = 20, min_pool = 1)\n",
    "\n",
    "decision_tree.fit(preprocessing.training_features, preprocessing.training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for both training data and test data\n",
    "training_predictions = decision_tree.predict(preprocessing.training_features)\n",
    "test_predictions = decision_tree.predict(preprocessing.validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2621,    0],\n",
       "       [   0, 2267]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on training and test data\n",
    "confusion_matrix(preprocessing.training_labels, training_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(preprocessing.training_labels, training_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[820,  58],\n",
       "       [ 81, 670]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(preprocessing.validation_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9146715776550031"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(preprocessing.validation_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Best Tree Depth\n",
    "Try different values of max_depth while keeping a constant min pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth to use is: 1\n",
      "The unique classes present in training data:[3.0, 5.0]\n",
      "Max depth to use is: 4\n",
      "The unique classes present in training data:[3.0, 5.0]\n",
      "Max depth to use is: 7\n",
      "The unique classes present in training data:[3.0, 5.0]\n",
      "Max depth to use is: 10\n",
      "The unique classes present in training data:[3.0, 5.0]\n",
      "Max depth to use is: 13\n",
      "The unique classes present in training data:[3.0, 5.0]\n",
      "Max depth to use is: 16\n",
      "The unique classes present in training data:[3.0, 5.0]\n",
      "Max depth to use is: 19\n",
      "The unique classes present in training data:[3.0, 5.0]\n",
      "Max depth to use is: 22\n",
      "The unique classes present in training data:[3.0, 5.0]\n",
      "\n",
      "The best test accuracy is 0.9183548189073051achieved using a max depth of 10.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree_depth_max = []\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "best_test_accuray = 0\n",
    "best_depth = 0\n",
    "max_depth = 25\n",
    "\n",
    "for max_depth_val in range(1, max_depth, 3):\n",
    "    print(\"Max depth to use is: {}\".format(max_depth_val))\n",
    "    tree_depth_max.append(max_depth_val)\n",
    "    root = Decision_Tree_Node(0)\n",
    "    decision_tree = Decision_Tree(root, max_depth = max_depth_val, min_pool = 1)\n",
    "    decision_tree.fit(preprocessing.training_features, preprocessing.training_labels)\n",
    "    \n",
    "    # predict for both training data and test data\n",
    "    training_predictions = decision_tree.predict(preprocessing.training_features)\n",
    "    test_predictions = decision_tree.predict(preprocessing.validation_features)\n",
    "\n",
    "    training_accuracy.append(accuracy_score(preprocessing.training_labels, training_predictions))\n",
    "    \n",
    "    test_acc = accuracy_score(preprocessing.validation_labels, test_predictions)\n",
    "    if test_acc > best_test_accuray:\n",
    "        best_test_accuray = test_acc\n",
    "        best_depth = max_depth_val\n",
    "    test_accuracy.append(test_acc)\n",
    "\n",
    "plt.plot(tree_depth_max, training_accuracy, 'g--', label=\"Training Accuracy\")\n",
    "plt.plot(tree_depth_max, test_accuracy, 'r--', label=\"Test Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "max_test_accuracy = max(test_accuracy)\n",
    "best_tree_depth = np.argmax(test_accuracy)\n",
    "\n",
    "print(\"\\nThe best test accuracy is \" + str(max_test_accuracy) + \"achieved using a max depth of \"+ str(best_depth) + \".\")\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
