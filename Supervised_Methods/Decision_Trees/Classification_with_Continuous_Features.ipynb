{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART Decision Tree for Optical Character Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build a decision tree to classify between two handwritten numbers, 3 and 5. The data set used is from Oregon State University, Fall 2018 CS 534 Machine Learning Course Implementation Assignment 3.<br><br>\n",
    "There are 100 features for each sample, corresponding to the top 100 principal components generated using PCA from the original sample features of size 28 * 28.<br><br>\n",
    "The training set contains 4888 samples. Each sample is a list of 101 values. The first column represents the digitâ€™s label  which is 3 or 5. The other 100 floating values are the PCA-generated features for this sample.<br><br>\n",
    "The validation set contains 1629 samples. This set will be used to tune the hyper parameters and select the best model.<br>\n",
    "No test set is provided.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Decision Tree algorithm does not need any scaling of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, raw_training_data_path, raw_validation_data_path):\n",
    "            \n",
    "            # Read the training and validation data. As the features are just PCA components, no need to observe the \n",
    "            # column names, so can proceed with numpy instead of pandas.\n",
    "            \n",
    "            self.raw_training_data = np.genfromtxt(raw_training_data_path, delimiter = \",\")\n",
    "            self.raw_validation_data = np.genfromtxt(raw_validation_data_path, delimiter = \",\")\n",
    "            \n",
    "            print(self.raw_training_data[0:10,:])\n",
    "            \n",
    "            # split training features and labels\n",
    "            self.training_features = self.raw_training_data[:, 1:]\n",
    "            self.training_labels = self.raw_training_data[:, 0]\n",
    "            \n",
    "            print(\"No of training samples: {}, No of training features: {}\".format(len(self.training_labels), self.training_features.shape[1]))\n",
    "            \n",
    "            \n",
    "            # split validation features and labels\n",
    "            self.validation_features = self.raw_validation_data[:, 1:]\n",
    "            self.validation_labels = self.raw_validation_data[:, 0]\n",
    "            \n",
    "            print(\"No of validation samples: {}\".format(len(self.validation_labels), ))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation with GINI Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree_Node:\n",
    "    \n",
    "    def __init__(self, X = None, y = None, depth = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.depth = depth\n",
    "        self.feature_index_with_optimal_split = None\n",
    "        self.threshold_for_optimal_split = None # since features are real values\n",
    "        self.is_leaf = False\n",
    "        self.class_probabilities = None\n",
    "\n",
    "\n",
    "class Decision_Tree(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    '''\n",
    "    Implementation of CART Decision Tree.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    max_depth : int, optional, default 100\n",
    "                The maximum depth of the deicison tree for early-stopping/ to avoid overfitting to outliers.\n",
    "    min_pool :  int, optional, default 10\n",
    "                The minimum number of samples at a node after split for early-stopping/ to avoid overfitting.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, root, max_depth = None, min_pool = None,):\n",
    "        \n",
    "        self.root = root\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_pool\n",
    "        self.classes = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def build_decision_tree(self, X, y, node, current_depth):\n",
    "\n",
    "        '''\n",
    "        Recursively builds the decision tree if the terminal conditions not met\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        X : training features\n",
    "        y : training labels\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # base cases of recursion\n",
    "        if current_depth >= self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "            \n",
    "        elif len(y) <= self.min_samples:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "        \n",
    "        elif len(np.unique(y)) == 1:\n",
    "            node.is_leaf = True\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            \n",
    "            \n",
    "            # find the best split and the feature that gives this best split\n",
    "            feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right = self.best_split(X, y)#, node.depth) \n",
    "            \n",
    "            \n",
    "            if feature_for_best_split is None:\n",
    "                # best_split() returns None if no feature gave a split that has less impurity than the parent\n",
    "                node.is_leaf = True\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Recursively build trees to the left and the right using the best features and the best split threshold\n",
    "                node.depth = current_depth\n",
    "                node.feature_index_with_optimal_split = feature_for_best_split\n",
    "                node.threshold_for_optimal_split = best_split_threshold\n",
    "                \n",
    "                node.left = Decision_Tree_Node(X_left, y_left, current_depth+1)\n",
    "                node.right = Decision_Tree_Node(X_right, y_right, current_depth+1)\n",
    "                \n",
    "                # when a node is encountered, calculate the class probabilities for each class at this node and store\n",
    "                node.left.class_probabilities = self.calculate_class_probabilities(y_left)\n",
    "                node.right.class_probabilities = self.calculate_class_probabilities(y_right)\n",
    "                \n",
    "                self.build_decision_tree(X_left, y_left, node.left, current_depth+1)\n",
    "                self.build_decision_tree(X_right, y_right, node.right, current_depth+1)\n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "    def gini_data(self,y): #####################MAKE THIS EFFICEINT\n",
    "        #count the Y equal to one\n",
    "        c_root_pos=np.count_nonzero(y==3) # The labels are 3 and 5, 3 is the first label in ascending order.\n",
    "        c_root_total = len(y)        \n",
    "        \n",
    "        gini_of_y =  1- (c_root_pos/c_root_total)**2 -((c_root_total-c_root_pos)/c_root_total)**2 \n",
    "        return gini_of_y\n",
    "    \n",
    "    def gini_after_split(self, y_copy, indices_where_change, i):\n",
    "        \n",
    "        '''\n",
    "        Calculate GINI impurity metric U after split\n",
    "        \n",
    "        U = (p^2) + (q^2 )\n",
    "        \n",
    "        where p = No of samples at this node with feature x below the node threshold/ No of samples at this node\n",
    "              q = No of samples at this node with feature x equal or above the node threshold/ No of samples at this node\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        '''\n",
    "        \n",
    "        CL_pos = np.count_nonzero(y_copy[:indices_where_change[i] + 1] == 3)\n",
    "        CL_total = len(y_copy[:indices_where_change[i] + 1])\n",
    "        CL_neg = CL_total - CL_pos\n",
    "\n",
    "        # calculate gain\n",
    "        prob1 = CL_pos / (CL_total)#sample_num+1/len(y)\n",
    "        prob2 = CL_neg / (CL_total)#(len(y) - (sample_num+1) )/len(y)\n",
    "\n",
    "        UAL = 1 - (prob1) ** 2 - (prob2) ** 2\n",
    "\n",
    "\n",
    "\n",
    "        CR_pos = np.count_nonzero(y_copy[indices_where_change[i] + 1:] == 3)\n",
    "        CR_total = len(y_copy[indices_where_change[i] + 1:])\n",
    "        CR_neg = CR_total - CR_pos\n",
    "        prob1 = CR_pos / (CR_total)\n",
    "        prob2 = CR_neg / (CR_total)\n",
    "        UAR = 1 - (prob1) ** 2 - (prob2) ** 2\n",
    "\n",
    "\n",
    "        pl = (CL_total) / len(y_copy)\n",
    "        pr = (CR_total) / len(y_copy)\n",
    "        \n",
    "        gini_after_split =  pl * UAL + pr * UAR\n",
    "        return gini_after_split\n",
    "    \n",
    "    \n",
    "    \n",
    "    def split_node(self, X, y, best_feature, max_benefit_val):\n",
    "    \n",
    "        left_X, right_X, left_y, right_y = [],[],[],[]\n",
    "\n",
    "        for row in range(0,X.shape[0]):\n",
    "            if X[row][best_feature] < max_benefit_val:\n",
    "                left_X.append(X[row])\n",
    "                left_y.append(y[row])\n",
    "            else:\n",
    "                right_X.append(X[row])\n",
    "                right_y.append(y[row])\n",
    "        \n",
    "        return np.array(left_X), np.array(right_X), np.array(left_y), np.array(right_y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def best_split(self, X_passed, y_passed):\n",
    "        '''\n",
    "        Divides the training samples into optimal splits according to the impurity metric\n",
    "        '''\n",
    "        numpy_X = []\n",
    "        numpy_y = []\n",
    "        \n",
    "        for row in range(0,len(X_passed)):\n",
    "            numpy_X.append(X_passed[row])\n",
    "            numpy_y.append(y_passed[row])\n",
    "        \n",
    "        X = np.array(numpy_X)\n",
    "        y = np.array(numpy_y)\n",
    "        \n",
    "        \n",
    "        gini_before_split = self.gini_data(y)\n",
    "\n",
    "        benefit_of_split = 0\n",
    "        \n",
    "        benefit_vals_for_one_feature = np.zeros(X.shape[1])\n",
    "        max_benefit_index_for_each_feature = np.zeros(X.shape[1])\n",
    "\n",
    "        \n",
    "        # for each feature, find the GINI gain at each threshold\n",
    "        for feature in range(0,X.shape[1]):\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            current_feature = X[:, feature]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # TRICK: Sort the training samples according to this feature's values\n",
    "            # and find GINI only at samples where the class label changes.\n",
    "            \n",
    "            \n",
    "            stacked = np.column_stack((y, current_feature)) # stack y_to_take and current_feature\n",
    "            sortedd = stacked[np.argsort(stacked[:, 1])]\n",
    "            \n",
    "            y_copy = sortedd[..., 0]\n",
    "            #indices_where_change = np.where(np.roll(y_copy,1)!=y_copy)[0]\n",
    "\n",
    "            indices_where_change = np.where(y_copy[:-1] != y_copy[1:])[0]\n",
    "                                            \n",
    "            for i in range(len(indices_where_change)):\n",
    "\n",
    "                gini_after_split = self.gini_after_split(y_copy, indices_where_change, i)\n",
    "\n",
    "                benefit_of_split = gini_before_split - gini_after_split\n",
    "\n",
    "\n",
    "                if benefit_of_split >  benefit_vals_for_one_feature[feature]:\n",
    "                    benefit_vals_for_one_feature[feature] = benefit_of_split  \n",
    "                    max_benefit_index_for_each_feature[feature] = indices_where_change[i]\n",
    "                   \n",
    "\n",
    "                \n",
    "                    \n",
    "\n",
    "        if benefit_of_split == 0:\n",
    "            #print(\"No good feature found.\")\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            \n",
    "            best_feature = np.argmax(benefit_vals_for_one_feature)\n",
    "            index_of_threhold_with_max_benefit_for_best_feature = max_benefit_index_for_each_feature[best_feature]\n",
    "            \n",
    "            # lets get the column of that feature and sort and get the threshold\n",
    "            max_benefit_feature = copy.deepcopy(X[:,best_feature])\n",
    "            max_benefit_feature.sort()\n",
    "            \n",
    "            # Athough we have threshold_index_for_best_split_in_indices_where_change, we need to find the\n",
    "            # corresponding index in the sorted max benefit feature.\n",
    "            \n",
    "\n",
    "            max_benefit_val = max_benefit_feature[int(index_of_threhold_with_max_benefit_for_best_feature)]\n",
    "            \n",
    "            \n",
    "            X_left, X_right, y_left, y_right = self.split_node(X, y, best_feature, max_benefit_val)\n",
    "            \n",
    "            \n",
    "\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "                        \n",
    "            # feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right\n",
    "            return best_feature, max_benefit_val, X_left, X_right, y_left, y_right\n",
    "                \n",
    "                        \n",
    "\n",
    "    def calculate_class_probabilities(self, y):\n",
    "        '''\n",
    "        Calculates the probability of each class at this node\n",
    "        '''\n",
    "        \n",
    "        class_probabilities = []\n",
    "        \n",
    "        for label in self.root.classes:\n",
    "            \n",
    "            if len(y[y==label]) == len(y) :\n",
    "                class_prob = 1\n",
    "            elif len(y[y==label]) == 0:\n",
    "                class_prob = 0\n",
    "            else:\n",
    "                class_prob = y[y==label].shape[0]/len(y)\n",
    "            \n",
    "            class_probabilities.append(class_prob)\n",
    "        \n",
    "        return class_probabilities\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def calculate_gini_impurity(self, labels):\n",
    "        '''\n",
    "        Calculate GINI impurity metric U at a node of the decision tree\n",
    "        \n",
    "        U = 1 - [(p^2) + (q^2 )]\n",
    "        \n",
    "        where p = No of samples at this node with feature x below the node threshold/ No of samples at this node\n",
    "              q = No of samples at this node with feature x equal or above the node threshold/ No of samples at this node\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        probabilities = []\n",
    "        for class_label in self.root.classes:\n",
    "            probability_of_class = len(labels[labels == class_label])/ len(labels)\n",
    "            probabilities.append(probability_of_class)\n",
    "        \n",
    "        impurity = 1 - sum([pow(elem,2) for elem in probabilities])\n",
    "        assert impurity <= 1\n",
    "        \n",
    "        return impurity\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    def predict_a_sample(self, x, node):\n",
    "        '''\n",
    "        Predicts a class label for the passed test sample by recursively calling a child node from the root upto\n",
    "        a leaf node of the learned decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        x : a single test sample features\n",
    "        node : a node of the learned decision tree\n",
    "        '''\n",
    "        \n",
    "        if node.is_leaf:\n",
    "            return node.class_probabilities\n",
    "        else:\n",
    "            \n",
    "            if x[node.feature_index_with_optimal_split] < node.threshold_for_optimal_split:\n",
    "                return self.predict_a_sample(x, node.left)\n",
    "            else:\n",
    "                return self.predict_a_sample(x, node.right)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts class labels for all test samples passed.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------------------\n",
    "        X : test samples passed as 2D arrays\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        test_preds = []\n",
    "        for test_sample in X:\n",
    "            class_probabilities = self.predict_a_sample(test_sample, self.root)\n",
    "            test_pred = np.argmax(class_probabilities)\n",
    "            if test_pred == 0:\n",
    "                \n",
    "                test_preds.append(3)\n",
    "            elif test_pred == 1:\n",
    "                test_preds.append(5)\n",
    "            else:\n",
    "                raise AssertionError\n",
    "        \n",
    "        return test_preds\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        '''\n",
    "        Train CART decision tree using GINI impurity.\n",
    "        \n",
    "        Parameters\n",
    "        ----------------\n",
    "        X : arraylike\n",
    "            The training samples in 2D format, corresponding to input features of each sample.\n",
    "        y : arraylike, optional, default None\n",
    "            The labels for each sample in X. Some algorithms do not require the label y while some (like Decisoin Tree) do.\n",
    "            Hence, keep y as optional.\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        self : Trained Decision Tree object\n",
    "               This method returns self for compatibility reasons with sklearn's other interfaces/ functionalities.\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.root = Decision_Tree_Node(X, y, 0)\n",
    "        self.root.classes = [3., 5.]#list(set(y)) # gives the set of unique elements in y.\n",
    "        self.root.classes.sort() # sorts in ascending order in-place if not already sorted\n",
    "        \n",
    "        print(\"The unique classes present in training data:{}\".format(self.root.classes))\n",
    "        \n",
    "        self.build_decision_tree(X = X, y = y, node = self.root, current_depth = 0)\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5.     -252.5092 1040.6188 ...  -21.6094  -32.602    25.619 ]\n",
      " [   5.     -684.5502 -368.7191 ...  -36.3467   26.6937  -17.564 ]\n",
      " [   3.      119.2934  638.9885 ...   14.7913   48.7926  -94.5664]\n",
      " ...\n",
      " [   5.      972.0162   77.9232 ...  -35.5166  -16.6162  -43.1298]\n",
      " [   5.     -255.5209  144.6523 ...   75.572    34.6369   24.2973]\n",
      " [   3.      217.1434  592.4619 ...  -25.7318   55.1806    4.9309]]\n",
      "No of training samples: 4888, No of training features: 100\n",
      "No of validation samples: 1629\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "training_data_path = \"data/pa3_train_reduced.csv\"\n",
    "validation_data_path = \"data/pa3_valid_reduced.csv\"\n",
    "\n",
    "preprocessing = Preprocessing(training_data_path, validation_data_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Analysis\n",
    "We can try and see if there exists just one feature that has good threholds that separates different classes in the pairplots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in range(1,len(preprocessing.training_features[0])):\n",
    "    feature_and_target = np.stack((  preprocessing.training_features[:,0], preprocessing.training_features[:,col] ), axis = 1)\n",
    "    #sns.pairplot(pd.DataFrame(feature_and_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique classes present in training data:[3.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "root = Decision_Tree_Node(0)\n",
    "decision_tree = Decision_Tree(root, max_depth = 20, min_pool = 1)\n",
    "\n",
    "\n",
    "decision_tree.fit(preprocessing.training_features, preprocessing.training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for both training data and test data\n",
    "training_predictions = decision_tree.predict(preprocessing.training_features)\n",
    "test_predictions = decision_tree.predict(preprocessing.validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2572,   49],\n",
       "       [  78, 2189]], dtype=int64)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on training and test data\n",
    "confusion_matrix(preprocessing.training_labels, training_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9740180032733224"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(preprocessing.training_labels, training_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[830,  48],\n",
       "       [ 90, 661]], dtype=int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(preprocessing.validation_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9152854511970534"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(preprocessing.validation_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
