{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest for Optimal Character Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build a random forest to classify between two handwritten numbers, 3 and 5. The data set used is from Oregon State University, Fall 2018 CS 534 Machine Learning Course Implementation Assignment 3.\n",
    "\n",
    "There are 100 features for each sample, corresponding to the top 100 principal components generated using PCA from the original sample features of size 28 * 28.\n",
    "\n",
    "The training set contains 4888 samples. Each sample is a list of 101 values. The first column represents the digitâ€™s label which is 3 or 5. The other 100 floating values are the PCA-generated features for this sample.\n",
    "\n",
    "The test set contains 1629 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, raw_training_data_path, raw_validation_data_path):\n",
    "            \n",
    "            # Read the training and validation data. As the features are just PCA components, no need to observe the \n",
    "            # column names, so can proceed with numpy instead of pandas.\n",
    "            \n",
    "            self.raw_training_data = np.genfromtxt(raw_training_data_path, delimiter = \",\")\n",
    "            self.raw_validation_data = np.genfromtxt(raw_validation_data_path, delimiter = \",\")\n",
    "            \n",
    "            print(self.raw_training_data[0:10,:])\n",
    "            \n",
    "            # split training features and labels\n",
    "            self.training_features = self.raw_training_data[:, 1:]\n",
    "            self.training_labels = self.raw_training_data[:, 0]\n",
    "            \n",
    "            print(\"No of training samples: {}, No of training features: {}\".format(len(self.training_labels), self.training_features.shape[1]))\n",
    "            \n",
    "            \n",
    "            # split validation features and labels\n",
    "            self.validation_features = self.raw_validation_data[:, 1:]\n",
    "            self.validation_labels = self.raw_validation_data[:, 0]\n",
    "            \n",
    "            print(\"No of validation samples: {}\".format(len(self.validation_labels), ))\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation\n",
    "This decision tree will facilitate the random forest algorithm later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree_Node:\n",
    "    \n",
    "    def __init__(self, X = None, y = None, depth = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.depth = depth\n",
    "        self.feature_index_with_optimal_split = None\n",
    "        self.threshold_for_optimal_split = None # since features are real values\n",
    "        self.is_leaf = False\n",
    "        self.class_probabilities = None\n",
    "\n",
    "\n",
    "class Decision_Tree(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    '''\n",
    "    Implementation of CART Decision Tree.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    max_depth : int, optional, default 100\n",
    "                The maximum depth of the deicison tree for early-stopping/ to avoid overfitting to outliers.\n",
    "    min_pool :  int, optional, default 10\n",
    "                The minimum number of samples at a node after split for early-stopping/ to avoid overfitting.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, root, max_depth = None, min_pool = None, no_of_bagging_features = 10):\n",
    "        \n",
    "        self.root = root\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_pool\n",
    "        self.classes = None\n",
    "        self.no_of_bagging_features = no_of_bagging_features\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def build_decision_tree(self, X, y, node, current_depth):\n",
    "\n",
    "        '''\n",
    "        Recursively builds the decision tree if the terminal conditions not met\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        X : training features\n",
    "        y : training labels\n",
    "        ''' \n",
    "        \n",
    "        # base cases of recursion\n",
    "        if current_depth >= self.max_depth:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "            \n",
    "        elif len(y) <= self.min_samples:\n",
    "            node.is_leaf = True\n",
    "            \n",
    "        \n",
    "        elif len(np.unique(y)) == 1:\n",
    "            node.is_leaf = True\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            \n",
    "            \n",
    "            # find the best split and the feature that gives this best split\n",
    "            feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right = self.best_split(X, y)#, node.depth) \n",
    "            \n",
    "            \n",
    "            if feature_for_best_split is None:\n",
    "                # best_split() returns None if no feature gave a split that has less impurity than the parent\n",
    "                node.is_leaf = True\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Recursively build trees to the left and the right using the best features and the best split threshold\n",
    "                node.depth = current_depth\n",
    "                node.feature_index_with_optimal_split = feature_for_best_split\n",
    "                node.threshold_for_optimal_split = best_split_threshold\n",
    "                \n",
    "                node.left = Decision_Tree_Node(X_left, y_left, current_depth+1)\n",
    "                node.right = Decision_Tree_Node(X_right, y_right, current_depth+1)\n",
    "                \n",
    "                # when a node is encountered, calculate the class probabilities for each class at this node and store\n",
    "                node.left.class_probabilities = self.calculate_class_probabilities(y_left)\n",
    "                node.right.class_probabilities = self.calculate_class_probabilities(y_right)\n",
    "                \n",
    "                self.build_decision_tree(X_left, y_left, node.left, current_depth+1)\n",
    "                self.build_decision_tree(X_right, y_right, node.right, current_depth+1)\n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "    def gini_data(self,y): #####################MAKE THIS EFFICEINT\n",
    "        '''\n",
    "        Calculates the GINI impurity of y before split.\n",
    "        '''\n",
    "        #count the Y equal to one\n",
    "        c_root_pos=np.count_nonzero(y==3) # The labels are 3 and 5, 3 is the first label in ascending order.\n",
    "        c_root_total = len(y)        \n",
    "        \n",
    "        gini_of_y =  1- (c_root_pos/c_root_total)**2 -((c_root_total-c_root_pos)/c_root_total)**2 \n",
    "        return gini_of_y\n",
    "  \n",
    "\n",
    "    def gini_after_split(self, y_copy, indices_where_change, i):\n",
    "        \n",
    "        '''\n",
    "        Calculates GINI impurity metric U after split of y_copy.\n",
    "        \n",
    "        U = (p^2) + (q^2 )\n",
    "        \n",
    "        where p = No of samples at this node with feature x below the node threshold/ No of samples at this node\n",
    "              q = No of samples at this node with feature x equal or above the node threshold/ No of samples at this node\n",
    "        \n",
    "        Parameters\n",
    "        -----------------\n",
    "        '''\n",
    "        \n",
    "        CL_pos = np.count_nonzero(y_copy[:indices_where_change[i] + 1] == 3)\n",
    "        CL_total = len(y_copy[:indices_where_change[i] + 1])\n",
    "        CL_neg = CL_total - CL_pos\n",
    "        prob1 = CL_pos / (CL_total)\n",
    "        prob2 = CL_neg / (CL_total)\n",
    "        UAL = 1 - (prob1) ** 2 - (prob2) ** 2\n",
    "\n",
    "\n",
    "\n",
    "        CR_pos = np.count_nonzero(y_copy[indices_where_change[i] + 1:] == 3)\n",
    "        CR_total = len(y_copy[indices_where_change[i] + 1:])\n",
    "        CR_neg = CR_total - CR_pos\n",
    "        prob1 = CR_pos / (CR_total)\n",
    "        prob2 = CR_neg / (CR_total)\n",
    "        UAR = 1 - (prob1) ** 2 - (prob2) ** 2\n",
    "\n",
    "\n",
    "        pl = (CL_total) / len(y_copy)\n",
    "        pr = (CR_total) / len(y_copy)\n",
    "        \n",
    "        gini_after_split =  pl * UAL + pr * UAR\n",
    "        \n",
    "        return gini_after_split\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def split_node(self, X, y, best_feature, max_benefit_val):\n",
    "        '''\n",
    "        Splits a node using the best feature that maximizes Information Gain.\n",
    "        '''\n",
    "        left_X, right_X, left_y, right_y = [],[],[],[]\n",
    "\n",
    "        for row in range(0,X.shape[0]):\n",
    "            if X[row][best_feature] < max_benefit_val:\n",
    "                left_X.append(X[row])\n",
    "                left_y.append(y[row])\n",
    "            else:\n",
    "                right_X.append(X[row])\n",
    "                right_y.append(y[row])\n",
    "        \n",
    "        return np.array(left_X), np.array(right_X), np.array(left_y), np.array(right_y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def bag_features(self, X):\n",
    "        '''\n",
    "        Generates the indices of the features to select to make splitting decisions.\n",
    "        '''\n",
    "        no_of_features = len(X[0])\n",
    "        \n",
    "        features_to_select_set = set()\n",
    "        features_to_select_list = []\n",
    "        \n",
    "        while len(features_to_select_list) < self.no_of_bagging_features:\n",
    "            \n",
    "            feature_selected = random.randint(0, no_of_features-1)\n",
    "            if feature_selected not in features_to_select_set:\n",
    "                features_to_select_list.append(feature_selected)\n",
    "        \n",
    "        return features_to_select_list\n",
    "            \n",
    "        \n",
    "        \n",
    "    def best_split(self, X_passed, y_passed):\n",
    "        '''\n",
    "        Divides the training samples into optimal splits according to the impurity metric\n",
    "        '''\n",
    "        numpy_X = []\n",
    "        numpy_y = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for row in range(0,len(X_passed)):\n",
    "            numpy_X.append(X_passed[row])\n",
    "            numpy_y.append(y_passed[row])\n",
    "        \n",
    "        X = np.array(numpy_X)\n",
    "        y = np.array(numpy_y)\n",
    "        \n",
    "        \n",
    "        gini_before_split = self.gini_data(y)\n",
    "\n",
    "        benefit_of_split = 0\n",
    "        \n",
    "        benefit_vals_for_one_feature = np.zeros(X.shape[1])\n",
    "        max_benefit_index_for_each_feature = np.zeros(X.shape[1])\n",
    "\n",
    "        \n",
    "        # FEATURE BAGGING\n",
    "        feature_indices_to_bag = self.bag_features(X)\n",
    "        \n",
    "               \n",
    "        # for each bagged feature, find the GINI gain at each threshold\n",
    "        for feature in range(0,len(feature_indices_to_bag)):\n",
    "            \n",
    "            current_feature = X[:, feature_indices_to_bag[feature]]\n",
    "            \n",
    "            # TRICK: Sort the training samples according to this feature's values\n",
    "            # and find GINI only at samples where the class label changes.\n",
    "            \n",
    "            \n",
    "            stacked = np.column_stack((y, current_feature)) # stack y_to_take and current_feature\n",
    "            sortedd = stacked[np.argsort(stacked[:, 1])]\n",
    "            \n",
    "            y_copy = sortedd[..., 0]\n",
    "            #indices_where_change = np.where(np.roll(y_copy,1)!=y_copy)[0]\n",
    "\n",
    "            indices_where_change = np.where(y_copy[:-1] != y_copy[1:])[0]\n",
    "            \n",
    "            # the values in indices_where_change are the indices after which the label changes.\n",
    "                                            \n",
    "            for i in range(len(indices_where_change)):\n",
    "\n",
    "                gini_after_split = self.gini_after_split(y_copy, indices_where_change, i)\n",
    "\n",
    "                benefit_of_split = gini_before_split - gini_after_split\n",
    "\n",
    "\n",
    "                if benefit_of_split >  benefit_vals_for_one_feature[feature]:\n",
    "                    benefit_vals_for_one_feature[feature] = benefit_of_split  \n",
    "                    max_benefit_index_for_each_feature[feature] = indices_where_change[i]\n",
    "                   \n",
    "                \n",
    "                    \n",
    "\n",
    "        if benefit_of_split == 0:\n",
    "            #print(\"No good feature found.\")\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            \n",
    "            best_feature = np.argmax(benefit_vals_for_one_feature)\n",
    "            index_of_threhold_with_max_benefit_for_best_feature = max_benefit_index_for_each_feature[best_feature]\n",
    "            \n",
    "            # lets get the column of that feature and sort and get the threshold\n",
    "            max_benefit_feature = copy.deepcopy(X[:,best_feature])\n",
    "            max_benefit_feature.sort()\n",
    "            \n",
    "            # Athough we have threshold_index_for_best_split_in_indices_where_change, we need to find the\n",
    "            # corresponding index in the sorted max benefit feature.\n",
    "            \n",
    "\n",
    "            max_benefit_val = max_benefit_feature[int(index_of_threhold_with_max_benefit_for_best_feature)+1]\n",
    "            \n",
    "            \n",
    "            X_left, X_right, y_left, y_right = self.split_node(X, y, best_feature, max_benefit_val)\n",
    "            \n",
    "                        \n",
    "            # feature_for_best_split, best_split_threshold, X_left, X_right, y_left, y_right\n",
    "            return best_feature, max_benefit_val, X_left, X_right, y_left, y_right\n",
    "                \n",
    "                        \n",
    "\n",
    "                \n",
    "    def calculate_class_probabilities(self, y):\n",
    "        '''\n",
    "        Calculates the probability of each class at this node\n",
    "        '''\n",
    "        \n",
    "        class_probabilities = []\n",
    "        \n",
    "        for label in self.root.classes:\n",
    "            \n",
    "            if len(y[y==label]) == len(y) :\n",
    "                class_prob = 1\n",
    "            elif len(y[y==label]) == 0:\n",
    "                class_prob = 0\n",
    "            else:\n",
    "                class_prob = y[y==label].shape[0]/len(y)\n",
    "            \n",
    "            class_probabilities.append(class_prob)\n",
    "        \n",
    "        return class_probabilities\n",
    "    \n",
    "         \n",
    "            \n",
    "    \n",
    "    def predict_a_sample(self, x, node):\n",
    "        '''\n",
    "        Predicts a class label for the passed test sample by recursively calling a child node from the root upto\n",
    "        a leaf node of the learned decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        x : a single test sample features\n",
    "        node : a node of the learned decision tree\n",
    "        '''\n",
    "        \n",
    "        if node.is_leaf:\n",
    "            return node.class_probabilities\n",
    "        else:\n",
    "            \n",
    "            if x[node.feature_index_with_optimal_split] < node.threshold_for_optimal_split:\n",
    "                return self.predict_a_sample(x, node.left)\n",
    "            else:\n",
    "                return self.predict_a_sample(x, node.right)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts class labels for all test samples passed.\n",
    "        \n",
    "        Parameters\n",
    "        ---------------------------\n",
    "        X : test samples passed as 2D arrays\n",
    "        '''        \n",
    "        test_preds = []\n",
    "        for test_sample in X:\n",
    "            class_probabilities = self.predict_a_sample(test_sample, self.root)\n",
    "            test_pred = np.argmax(class_probabilities)\n",
    "            if test_pred == 0:\n",
    "                \n",
    "                test_preds.append(3)\n",
    "            elif test_pred == 1:\n",
    "                test_preds.append(5)\n",
    "            else:\n",
    "                raise AssertionError\n",
    "        \n",
    "        return test_preds\n",
    "        \n",
    "       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        '''\n",
    "        Train CART decision tree using GINI impurity.\n",
    "        \n",
    "        Parameters\n",
    "        ----------------\n",
    "        X : arraylike\n",
    "            The training samples in 2D format, corresponding to input features of each sample.\n",
    "        y : arraylike, optional, default None\n",
    "            The labels for each sample in X. Some algorithms do not require the label y while some (like Decisoin Tree) do.\n",
    "            Hence, keep y as optional.\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        self : Trained Decision Tree object\n",
    "               This method returns self for compatibility reasons with sklearn's other interfaces/ functionalities.\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.root = Decision_Tree_Node(X, y, 0)\n",
    "        self.root.classes = list(set(y)) # gives the set of unique elements in y.\n",
    "        self.root.classes.sort() # sorts in ascending order in-place if not already sorted\n",
    "        #print(\"The unique classes present in training data:{}\".format(self.root.classes))\n",
    "        self.build_decision_tree(X = X, y = y, node = self.root, current_depth = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Forest:\n",
    "    \n",
    "    def __init__(self, train_X, train_y, test_X, test_y, max_depth, min_pool, num_of_trees, subsample_size, num_of_features_for_bagging):\n",
    "        '''\n",
    "        Initializes the parameters of the random forest.\n",
    "        '''\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.test_X = test_X\n",
    "        self.test_y = test_y\n",
    "        \n",
    "        self.unique_classes = np.unique(self.train_y)\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_pool = min_pool\n",
    "        \n",
    "        self.num_of_trees = num_of_trees\n",
    "        self.subsample_size = subsample_size\n",
    "        self.num_of_features_for_bagging = num_of_features_for_bagging\n",
    "        \n",
    "        self.trees = []\n",
    "    \n",
    "\n",
    "    def subsample_training_data(self):\n",
    "        '''\n",
    "        Generates a subsample of size self.sample_size from the training data.\n",
    "        '''\n",
    "        \n",
    "        training_X_subsample = []\n",
    "        training_y_subsample = []\n",
    "        \n",
    "        for row in range(0, int(self.subsample_size)):\n",
    "            index = random.randint(0, len(self.train_y)-1)\n",
    "            \n",
    "            training_X_subsample.append(self.train_X[index])\n",
    "            training_y_subsample.append(self.train_y[index])\n",
    "        \n",
    "        X = np.array(training_X_subsample)\n",
    "        y = np.array(training_y_subsample)\n",
    "        \n",
    "        return X,y\n",
    "        \n",
    "    \n",
    "    \n",
    "    def bagging_vote(self, sample):\n",
    "        '''\n",
    "        Predicts a class label for a sample. The prediction is the class label that has the maximum number of votes \n",
    "        from the different trees in the random forest.\n",
    "        '''\n",
    "        classes = dict()\n",
    "        for class_ in self.unique_classes:\n",
    "            classes[int(class_)] = 0\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            prediction  = tree.predict([sample])[0] #unsqueeze using [0]\n",
    "            classes[prediction] += 1\n",
    "        \n",
    "        major_class = None\n",
    "        major_class_vote = 0\n",
    "        \n",
    "        for class_key in classes:\n",
    "            class_val = classes[class_key]\n",
    "            if class_val > major_class_vote:\n",
    "                major_class = class_key\n",
    "                major_class_vote = class_val\n",
    "        \n",
    "        return major_class\n",
    "                \n",
    "    \n",
    "    \n",
    "    def predict_a_sample(self, features):\n",
    "        '''\n",
    "        Predicts a class label for a sample.\n",
    "        '''\n",
    "        prediction = self.bagging_vote(features)\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, features):\n",
    "        '''\n",
    "        Predicts class label a set of samples.\n",
    "        '''\n",
    "        predictions = []\n",
    "        for idx in range(len(features)):\n",
    "            predictions.append(self.predict_a_sample(features[idx]))\n",
    "        \n",
    "        return predictions\n",
    "                \n",
    "        \n",
    "    \n",
    "\n",
    "    def train_decision_tree(self, X, y, max_depth, min_pool):\n",
    "        '''\n",
    "        Creates and trains a decision tree of the random forest based on the data set and parameters passed.\n",
    "        '''\n",
    "        root_node = Decision_Tree_Node(X,y,max_depth)\n",
    "        \n",
    "        decision_tree = Decision_Tree(root_node, max_depth, min_pool, self.num_of_features_for_bagging)\n",
    "\n",
    "        decision_tree.fit(X, y)\n",
    "        \n",
    "        return decision_tree\n",
    "    \n",
    "    \n",
    "\n",
    "    def fit(self):\n",
    "        '''\n",
    "        Trains all decision trees of the random forest.\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        for tree_num in range(0, self.num_of_trees):\n",
    "            \n",
    "            subsample_X, subsample_y = self.subsample_training_data()\n",
    "            \n",
    "            \n",
    "            tree = self.train_decision_tree(X = subsample_X, y = subsample_y, max_depth = self.max_depth, min_pool = self.min_pool)\n",
    "            self.trees.append(tree)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5.     -252.5092 1040.6188 ...  -21.6094  -32.602    25.619 ]\n",
      " [   5.     -684.5502 -368.7191 ...  -36.3467   26.6937  -17.564 ]\n",
      " [   3.      119.2934  638.9885 ...   14.7913   48.7926  -94.5664]\n",
      " ...\n",
      " [   5.      972.0162   77.9232 ...  -35.5166  -16.6162  -43.1298]\n",
      " [   5.     -255.5209  144.6523 ...   75.572    34.6369   24.2973]\n",
      " [   3.      217.1434  592.4619 ...  -25.7318   55.1806    4.9309]]\n",
      "No of training samples: 4888, No of training features: 100\n",
      "No of validation samples: 1629\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "training_data_path = \"data/pa3_train_reduced.csv\"\n",
    "validation_data_path = \"data/pa3_valid_reduced.csv\"\n",
    "\n",
    "preprocessing = Preprocessing(training_data_path, validation_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 15\n",
    "min_pool = 1\n",
    "\n",
    "total_training_samples = len(preprocessing.training_labels)\n",
    "sample_size_for_bagging = total_training_samples\n",
    "total_features_len = len(preprocessing.training_features[0])\n",
    "num_of_features_for_bagging = total_features_len/5\n",
    "\n",
    "\n",
    "num_trees_for_random_forest = [1,2,5, 10, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for num_trees in num_trees_for_random_forest:\n",
    "\n",
    "    random_forest = Random_Forest(preprocessing.training_features,\n",
    "                                  preprocessing.training_labels,\n",
    "                                  preprocessing.validation_features,\n",
    "                                  preprocessing.validation_labels,\n",
    "                                  max_depth,\n",
    "                                  min_pool,\n",
    "                                  num_trees,\n",
    "                                  sample_size_for_bagging,\n",
    "                                  num_of_features_for_bagging\n",
    "\n",
    "                                 )\n",
    "    random_forest.fit()\n",
    "    \n",
    "    # predict for both training data and test data\n",
    "    training_predictions = random_forest.predict(preprocessing.training_features)\n",
    "    training_accuracy.append(accuracy_score(preprocessing.training_labels, training_predictions))\n",
    "    \n",
    "    \n",
    "    test_predictions = random_forest.predict(preprocessing.validation_features)\n",
    "    test_accuracy.append(accuracy_score(preprocessing.validation_labels, test_predictions))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_trees_for_random_forest, training_accuracy, 'g--', label=\"Training Accuracy\")\n",
    "plt.plot(num_trees_for_random_forest, test_accuracy, 'r--', label=\"Test Accuracy\")\n",
    "plt.xlabel(\"No of Trees in Random Forest\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
