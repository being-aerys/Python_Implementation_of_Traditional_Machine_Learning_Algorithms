{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression<br>\n",
    "\n",
    "The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data.<br>\n",
    "\n",
    "The columns/ features are as follows: longitude, latitude, housing median age, total_rooms, total_bedrooms, population, households, median_income, median house value, ocean_proximity.<br> The goal here is to predict the median house value based on the other features.<br>\n",
    "\n",
    "This dataset is a modified version of the California Housing dataset available from:\n",
    "Lu√≠s Torgo's page (University of Porto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    #load the data from the file and split into train and test sets\n",
    "    def fetch_train_test_data(self, path):\n",
    "        \n",
    "        raw_features_and_labels = pd.read_csv(path, sep = \",\",)\n",
    "        #handle categorical features before splitting the data set.\n",
    "        features_labels = [ \"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \n",
    "                   \"households\", \"median_income\", \"ocean_proximity\"]\n",
    "        target_label = [\"median_house_value\"]\n",
    "        features = raw_features_and_labels[features_labels]\n",
    "        labels = raw_features_and_labels[target_label]\n",
    "        #Get categorical features' names\n",
    "        list_of_categorical_feature_names = []\n",
    "\n",
    "        for item in features.columns:\n",
    "            if features.dtypes[item] == \"object\":\n",
    "                list_of_categorical_feature_names.append(item)\n",
    "        \n",
    "        features_with_categorical_features_one_hot_encoded = pd.DataFrame(self.one_hot_encode_categorical_features(features, list_of_categorical_feature_names))\n",
    "        #merge the features and labels before splitting\n",
    "        features_and_labels = features_with_categorical_features_one_hot_encoded.join(labels)\n",
    "        train_set, test_set = train_test_split(features_and_labels, test_size = 0.2, random_state = 42)    \n",
    "        return train_set, test_set\n",
    "      \n",
    "    \n",
    "    #Handle Categorical Features\n",
    "    def one_hot_encode_categorical_features(self, features, list_of_column_names_with_categorical_data ):\n",
    "        \n",
    "        print(\"Before one-hot encoding, the shape of the features: \", features.shape)\n",
    "        transformer = ColumnTransformer(transformers=[(\"OneHot\",                    # Just a name\n",
    "                                         OneHotEncoder(),                           # The transformer class\n",
    "                                         list_of_column_names_with_categorical_data # The column(s) to be applied on.\n",
    "                                             )\n",
    "                                            ],\n",
    "                                            remainder='passthrough' # donot apply anything to the remaining columns\n",
    "                                            )\n",
    "        features_with_categorical_features_one_hot_encoded = pd.DataFrame(transformer.fit_transform(features))\n",
    "        #Note that we lose column names while using the column transformer. Also, the categorical features' new \n",
    "        #columns are added to the front of the data frame, instead of where the categorical feature was.\n",
    "        print(\"After one-hot encoding, the shape of the features: \", features_with_categorical_features_one_hot_encoded.shape)\n",
    "        return features_with_categorical_features_one_hot_encoded\n",
    "    \n",
    "    \n",
    "    \n",
    "    #preprocess training data        \n",
    "    def preprocess_training_data(self, training_features_and_labels):\n",
    "        \n",
    "        num_features = training_features_and_labels.shape[1]       \n",
    "        training_features = training_features_and_labels.iloc[:, 0:num_features-1]\n",
    "        training_targets = training_features_and_labels.iloc[:,num_features-1]\n",
    "        \n",
    "        #training_features_and_labels.shape, training_features.shape, training_targets.shape)\n",
    "        #(16512, 14)                         (16512, 13)              (16512,)\n",
    "        \n",
    "        #normalize the features\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(training_features)\n",
    "        scaled_features = pd.DataFrame(scaler.transform(training_features))\n",
    "        #bias of all ones\n",
    "        bias_feature = pd.DataFrame(np.ones(len(training_features)).reshape((len(training_features),1)))    \n",
    "        print(\"No. of features before adding the bias feature: \", training_features.shape)   \n",
    "        #stack the new bias feature to the left of the regular features using the axis argument\n",
    "        features_with_bias = pd.concat([bias_feature, scaled_features], axis = 1)\n",
    "        print(\"No. of features after adding the bias feature: \", features_with_bias.shape)\n",
    "        \n",
    "\n",
    "        return scaler, features_with_bias, training_targets\n",
    "                \n",
    "\n",
    "        \n",
    "    \n",
    "    #preprocess testing data\n",
    "    def preprocess_test_data(self, scaler, testing_features_and_labels):\n",
    "        \n",
    "        num_features = testing_features_and_labels.shape[1]           \n",
    "        testing_features = testing_features_and_labels.iloc[:, 0:num_features-1]\n",
    "        testing_targets = testing_features_and_labels.iloc[:,num_features-1]\n",
    "            \n",
    "        #transform the features using the parameters from the training data\n",
    "        scaled_features =  pd.DataFrame(scaler.transform(testing_features))\n",
    "        #bias of all ones\n",
    "        bias_feature = pd.DataFrame(np.ones(len(scaled_features)).reshape((len(scaled_features),1)))\n",
    "\n",
    "        #stack the new bias feature to the left of the regular features using the axis argument\n",
    "        features_with_bias = pd.concat([bias_feature, scaled_features], axis = 1)\n",
    "        return features_with_bias, testing_targets\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualization:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def display_correlation_among_features(self, features):\n",
    "        pass\n",
    "        \n",
    "    def display_distributions_of_features(self, features):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Gradient Descent Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradient_Descent:\n",
    "    \n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    \n",
    "    def update_parameters(self, Theta, X, Y):\n",
    "        \n",
    "        X_times_Theta = X.dot(Theta)\n",
    "        X_time_Theta_minus_Y = X_times_Theta - Y\n",
    "        X_transpose = X.T\n",
    "        gradient_of_loss = X_transpose.dot(X_time_Theta_minus_Y)\n",
    "        new_weights = Theta - self.learning_rate * gradient_of_loss\n",
    "        return new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    \n",
    "    def __init__(self, max_iters, features_train, targets_train, features_test, targets_test, scaler_used_for_features):\n",
    "        self.writer = SummaryWriter()\n",
    "        self.max_iters = max_iters\n",
    "        self.features_train = features_train\n",
    "        self.targets_train = targets_train\n",
    "        self.features_test = features_test\n",
    "        self.scaler_used_for_features = scaler_used_for_features\n",
    "    \n",
    "    def calculate_regression_loss(self, X, Theta, Y):\n",
    "        \n",
    "        X_times_Theta = X.dot(Theta)\n",
    "        X_times_Theta_minus_Y = X_times_Theta - Y\n",
    "        loss = (1/ (2 * len(Y))) * (X_times_Theta_minus_Y.T).dot(X_times_Theta_minus_Y)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "class Linear_Regression(Regression):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gradient_descent = Gradient_Descent()\n",
    "                        \n",
    "    def batch_gradient_descent_train(self):\n",
    "        \n",
    "        #Initialize random weights and biases\n",
    "        weights = pd.DataFrame(np.random.randint(0,100,size=self.features_train.shape[1])).reshape(1, self.features_train.shape[1])\n",
    "        \n",
    "        epoch = 0\n",
    "        \n",
    "        while iteration < max_iters:\n",
    "            print(\"Training Iteration No: \", iteration)\n",
    "            \n",
    "            loss_in_epoch = 0\n",
    "            \n",
    "            #Get a shuffled list of indices of training data. (Not required when using batch gradient descent though)\n",
    "            shuffled_indices = np.random.permutation(len(self.features_train))\n",
    "            \n",
    "            weights = self.gradient_descent.update_parameters(weights, self.features_train, self.targets_train)\n",
    "            \n",
    "            training_loss_in_epoch = self.calculate_regression_loss(self.features_train, weights, self.targets_train)\n",
    "            \n",
    "            testing_loss_in_epoch = self.calculate_regression_loss(self.features_test, weights, self.targets_test)\n",
    "            \n",
    "            self.writer.addscalar(\"Linear Regression Batch GD Training Loss\",training_loss_in_epoch, iter)\n",
    "            self.writer.addscalar(\"Linear Regression Batch GD Testing Loss\",testing_loss_in_epoch, iter)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                        \n",
    "                        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before one-hot encoding, the shape of the features:  (20640, 9)\n",
      "After one-hot encoding, the shape of the features:  (20640, 13)\n",
      "No. of features before adding the bias feature:  (16512, 13)\n",
      "No. of features after adding the bias feature:  (16512, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_csv_path = r\"C:\\Users\\Being_Aerys\\PycharmProjects\\Machine_Learning_Algorithms_Collection\\Supervised_Methods\\Linear_Regression\\data\\housing.csv\"\n",
    "preprocessing = Preprocessing()\n",
    "\n",
    "train_set, test_set = preprocessing.fetch_train_test_data(raw_csv_path)\n",
    "\n",
    "scaler, normalized_training_data, training_labels = preprocessing.preprocess_training_data(train_set)\n",
    "#Lets summarize the information of the training data set.\n",
    "normalized_training_data.head()\n",
    "\n",
    "test_features, test_labels = preprocessing.preprocess_test_data(scaler, test_set)\n",
    "\n",
    "max_iters = 25\n",
    "\n",
    "linear_regression = Linear_Regression(self, max_iters, normalized_training_data, training_labels, test_features, targets_test, scaler_used_for_features)\n",
    "\n",
    "linear_regression.batch_gradient_descent_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that one of the features \"ocean_proiximity\" is a categorical features. If a categorical feature is has two only two classes, they can be encoded as binary features. However, if there are more than 2 categories, they can be either label encoded or one-hot encoded. Label encoding means assigning an integer to each class. However, consider a case where the feature represents the color of the eyes of an individual. If the labelling is done as black eyes --> 1, brown eyes --> 1, blue eyes --> 2, green eyes --> 3, the algorithm that takes in this information will interpret as one color being \"larger\" than the other color, which is not what we intend to tell. Hence, in such cases, one-hot encoding comes to our rescue which does not present such bias to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
