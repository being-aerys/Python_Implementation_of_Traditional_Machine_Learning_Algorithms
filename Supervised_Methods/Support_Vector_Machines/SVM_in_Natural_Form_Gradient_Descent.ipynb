{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Species Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.numerical_features_name_list_including_labels = None\n",
    "        self.categorical_features_namelist = None\n",
    "        #self.scaler_used_for_features = scaler_used_for_features\n",
    "        self.labels = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Custom train_test_split indices generator\n",
    "    def train_test_split_indices(self, len_data, test_split_size):\n",
    "        \n",
    "        #Note that if a sample is to be dropped because of too many missing features, it should be done before this step.\n",
    "        np.random.seed(42)\n",
    "        shuffled_data_indices = np.random.permutation(len_data)\n",
    "        total_test_data = int(test_split_size * len_data)\n",
    "        testing_indices = shuffled_data_indices[:total_test_data]\n",
    "        training_indices = shuffled_data_indices[total_test_data:]\n",
    "        return training_indices, testing_indices\n",
    "    \n",
    "    \n",
    "    def set_numerical_and_categorical_feature_names(self, dataframe):\n",
    "        \n",
    "        #get the names of the numeric feature columns only\n",
    "        self.numerical_features_name_list_including_labels = dataframe.select_dtypes(include=np.number).columns.tolist()\n",
    "        #get the names of the categorical feature columns only\n",
    "        list_of_categorical_feature_names = []\n",
    "        for feature in dataframe.columns:\n",
    "            if dataframe.dtypes[feature] == \"object\":\n",
    "                \n",
    "                list_of_categorical_feature_names.append(feature)\n",
    "        self.categorical_features_namelist = list_of_categorical_feature_names\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla SVM Implementation with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-b61b785df691>, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-b61b785df691>\"\u001b[1;36m, line \u001b[1;32m62\u001b[0m\n\u001b[1;33m    assert X.ndim == 2,\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class VanillaSVM(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    Implementation of Vanilla Support Vector Machine algorithm in its natural form.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    c : float, optional, default 1.0\n",
    "        The slack penalty, a tradeoff between maximizing the margin and minimizing the classification errors.\n",
    "    learning_rate : float, optional, default 0.001\n",
    "                    The step size to update the parameters of the model with at each update.\n",
    "    kernel : kernel object, optional, default None\n",
    "             The type of kernel object used by this SVM        \n",
    "    max_epochs : int, optional, default 100\n",
    "                 The number of training iterations to perform by the SVM object.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def _init_(self, learning_rate = 0.001, c = 1.0, kernel = None, max_epochs = 100):\n",
    "        \n",
    "        self.c = c\n",
    "        self.learning_rate = learning_rate\n",
    "        self.kernel = kernel\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.bias_ = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "\n",
    "        '''\n",
    "        Trains SVM in its natural form.\n",
    "        \n",
    "        Parameters\n",
    "        ----------------\n",
    "        X : arraylike\n",
    "            The training samples in 2D format, corresponding to input features of each sample\n",
    "            to train the vanilla SVM model in its natural form.\n",
    "        y : arraylike, optional, default None\n",
    "            The labels for each sample in X. Some algorithms do not require the label y while some (like SVM) do.\n",
    "            Hence, keep y as optional.\n",
    "        \n",
    "        Returns\n",
    "        ----------------\n",
    "        self : Trained VanillaSVM object\n",
    "               This method returns self for compatibility reasons with sklearn's other interfaces/ functionalities.\n",
    "         \n",
    "        '''\n",
    "        \n",
    "        # 1.check that the estimator's hyperparameters are set correctly.\n",
    "        assert type(self.c) == float or type(self.c) == int, \"Data type of parameter c is invalid.\"\n",
    "        assert self.c >= 0 and self.c <= sys.maxsize, \"Value of parameter c is out of possible range.\"\n",
    "        assert self.max_epochs > 0 and self.maxsize <= sys.maxsize, \"Value of max epochs is invalid.\"\n",
    "        \n",
    "        # 2.check the shape of X, it should be a 2D array to be compatible with sklearn methods later\n",
    "        #This is done by default in _validate_data() method of Base Estimator\n",
    "        if y is not None:\n",
    "            X, y = self._validate_data(X, y)\n",
    "        else:\n",
    "            X = self._validate_data(X)\n",
    "        \n",
    "        # 3.perform the trianing of SVM here. Here, we will use SGD instead of BGD or MBGD.\n",
    "        self.weights_ = np.random.normal(0, 1, X.shape[-1])\n",
    "        \n",
    "        iter = 0\n",
    "        np.random.seed(42)\n",
    "        for iter in range(self.max_epochs):           \n",
    "            \n",
    "            #shuffle the indices for random updates at each epoch during Stochastic Gradient Descent update\n",
    "            random_indices_ = np.random.permutation(X.shape[0])\n",
    "            \n",
    "            for index_ in range(len(random_indices_)):\n",
    "                \n",
    "                gradeint_w = None\n",
    "                gradient_b = None\n",
    "                #calculate the gradient of the cost wrt to each parameter/ weight and bias\n",
    "                #There are two parts to our gradient.\n",
    "                gradient_w_1 = self.weights_ #first part is wj for each feature j, which when taken all components at once is w itself.\n",
    "                \n",
    "                gradient_w_2 = None\n",
    "                \n",
    "                if y[random_indices_[index_]] * (np.dot(self.weights_, X[random_indices_[index_]]) + self.bias_) >= 1:\n",
    "                    gradient_w_2 = 0\n",
    "                    gradient_b = 0\n",
    "                else:\n",
    "                    gradient_w_2 = - * ( y[random_indices_[index_]] * X[random_indices_[index_]])\n",
    "                    gradient_b = - * y[random_indices_[index_]]\n",
    "                    \n",
    "                gradeint_w = gradient_w_1 + gradient_w_2\n",
    "                  \n",
    "                \n",
    "                #update the parameters/ weights\n",
    "                self.weights_ = self.weights_ - self.learning_rate * (self.weights_ + (self.c * gradient_w))\n",
    "                self.bias_ = self.bias_ - self.learning_rate * (self.c * (gradient_b))\n",
    "                \n",
    "            \n",
    "            #If the error on validation set stops dropping, stop training to prevent overfitting.\n",
    "            #Or continue training and save the model that gives the maximum validation accuracy among all epochs\n",
    "            '''TODO'''\n",
    "                                                                    \n",
    "   \n",
    "            #update the iteration count\n",
    "            iters += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    def predict_proba(self, X):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
