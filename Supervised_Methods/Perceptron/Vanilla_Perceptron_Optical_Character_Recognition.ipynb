{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Perceptron for Optical Character Recognition\n",
    "\n",
    "In the Optical Character Recognition (OCR) we seek to predict, for a given handwritten digit's image, the corresponding number. Here, we simplify the OCR into a binary classification task. Specifically, we consider predictions for only two numbers 3 and 5. \n",
    "## Training Data\n",
    "Includes 4888 rows (samples). Each sample/ row is in fact the target digit and a list of flattened gray-scale values from a 2d digital handwritten image with shape 28 × 28. -->  1 + 784  = 785 values. The first number is the digit’s label which is 3 or 5. The other 784 floating values are the grayscale values.\n",
    "## Validation Data\n",
    " Includes 1629 rows. Each row obeys the same format given for the train set. This set will be used to select your best trained model.\n",
    " ## Test Data\n",
    " Includes 1629 rows. Each row contains only 784 numbers. The label column (the first column in the trianing and the validatin data) is omitted from each row here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def convert_into_perceptron_binary_classes(self, df):\n",
    "        #change each row based on the condition\n",
    "        \n",
    "        final = []\n",
    "        \n",
    "        for idx in range(len(df)):\n",
    "            if df[idx] == 3:\n",
    "                final.append(1)\n",
    "            else:\n",
    "                final.append(-1)\n",
    "        return pd.DataFrame(final)\n",
    "        \n",
    "    \n",
    "    def preprocess_train_csv(self, path):\n",
    "        raw_data = pd.read_csv(path, sep = \",\", header = None)\n",
    "        features = raw_data.iloc[:, 1:]\n",
    "        \n",
    "        targets = raw_data.loc[:,0]\n",
    "        \n",
    "        #normalize the features\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(features)\n",
    "        scaled_features = scaler.transform(features) \n",
    "        \n",
    "        #bias of all ones\n",
    "        bias_feature = pd.DataFrame(np.ones(len(features)).reshape((len(features),1)))\n",
    "        \n",
    "        #stack the new bias feature to the left of the regular features using the axis argument\n",
    "        features_with_bias = pd.concat([bias_feature, features], axis = 1) #now has 785 features including the bias\n",
    "        \n",
    "        #convert the targets into -1 and +1 for perceptron operation        \n",
    "        targets = self.convert_into_perceptron_binary_classes(targets)\n",
    "        \n",
    "        return scaler, features_with_bias, targets\n",
    "    \n",
    "    def preprocess_validation_csv(self, scaler, path):\n",
    "        raw_data = pd.read_csv(path, sep = \",\", header = None)\n",
    "        features = raw_data.iloc[:, 1:]\n",
    "        targets = raw_data.loc[:,0]\n",
    "        #transform the features using the parameters from the training data\n",
    "        scaled_features = scaler.transform(features)\n",
    "        \n",
    "        #bias of all ones\n",
    "        bias_feature = pd.DataFrame(np.ones(len(features)).reshape((len(features),1)))\n",
    "        \n",
    "        #stack the new bias feature to the left of the regular features using the axis argument\n",
    "        features_with_bias = pd.concat([bias_feature, features], axis = 1) #now has 785 features including the bias\n",
    "        \n",
    "        \n",
    "        \n",
    "        #convert the targets into -1 and +1 for perceptron operation\n",
    "        targets = self.convert_into_perceptron_binary_classes(targets)\n",
    "        \n",
    "        return  features_with_bias, targets\n",
    "        \n",
    "        \n",
    "        \n",
    "    def preprocess_test_csv(self, scaler, path):\n",
    "        features = pd.read_csv(path, sep = \",\", header = None)\n",
    "        \n",
    "        \n",
    "        #transform the features using the parameters from the training data\n",
    "        scaled_features =  pd.DataFrame(scaler.transform(features))\n",
    "        #bias of all ones\n",
    "        bias_feature = pd.DataFrame(np.ones(len(scaled_features)).reshape((len(scaled_features),1)))\n",
    "        \n",
    "        #stack the new bias feature to the left of the regular features using the axis argument\n",
    "        features_with_bias = pd.concat([bias_feature, scaled_features], axis = 1) #now has 785 features including the bias\n",
    "        \n",
    "        return features_with_bias\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Perceptron:\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "    \n",
    "    \n",
    "#     def calculate_training_data_accuracy(num_of_mistakes, total_samples):\n",
    "        \n",
    "#         return (num_mistakes * 1./total_samples)\n",
    "    \n",
    "    \n",
    "class Vanilla_Perceptron():\n",
    "    \n",
    "    def __init__(self, max_iters, features_train, targets_train, features_validation, targets_validation, features_test):\n",
    "        \n",
    "        self.writer = SummaryWriter()\n",
    "        \n",
    "        self.max_iters = max_iters\n",
    "        self.features_train = features_train\n",
    "        self.targets_train = targets_train\n",
    "        self.features_validation = features_validation\n",
    "        self.targets_validation = targets_validation\n",
    "        self.features_test = features_test\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        #Initialize the weights and biases randomly\n",
    "        weights = pd.DataFrame(np.zeros(self.features_train.shape[1]).reshape((1,self.features_train.shape[1] )))\n",
    "        \n",
    "        \n",
    "        iteration = 0\n",
    "        max_validation_accuracy_iteration_num = None\n",
    "        max_accuracy= 0\n",
    "        \n",
    "        while iteration < self.max_iters:\n",
    "            print(\"Training Epoch: \", iteration)\n",
    "            \n",
    "            mistakes_in_epoch = 0\n",
    "            \n",
    "            #Get a shuffled list of indices of training data\n",
    "            shuffled_indices = np.random.permutation(len(self.features_train))\n",
    "            \n",
    "            for idx in range(len(shuffled_indices)):\n",
    "                \n",
    "                W_times_X = weights.dot(self.features_train.iloc[idx,:])\n",
    "                \n",
    "                if self.targets_train.iloc[idx,:].values[0] * W_times_X.values[0] <= 0.:\n",
    "                    #using  \".values[0]\" to obtain the scalar value from the data frame.\n",
    "                    \n",
    "                    #mistake occurred\n",
    "                    mistakes_in_epoch += 1\n",
    "                    #update the perceptron weights\n",
    "                    weights = weights + self.targets_train.iloc[idx,:].values[0] * self.features_train.iloc[idx,:]\n",
    "                \n",
    "                #else the perceptron classified this sample correctly\n",
    "            \n",
    "            accuracy_in_epoch = 1 - ((mistakes_in_epoch*1.)/len(self.features_train))\n",
    "            self.writer.add_scalar(\"Vanilla Perceptron Training Data Accuracy\",accuracy_in_epoch, iteration)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Validate()\n",
    "            current_weights = copy.deepcopy(weights)\n",
    "            num_of_validation_errors = self.validate(current_weights)\n",
    "            \n",
    "            validation_accuracy_in_epoch = 1 - ( num_of_validation_errors * 1./len(self.features_validation))\n",
    "            print(\"Validation Accuracy: \", validation_accuracy_in_epoch)\n",
    "            self.writer.add_scalar(\"Vanialla Perceptron Validation Data Accuracy\", validation_accuracy_in_epoch, iteration)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if validation_accuracy_in_epoch > max_accuracy:\n",
    "                max_accuracy = validation_accuracy_in_epoch\n",
    "                max_validation_accuracy_iteration_num = iteration\n",
    "            \n",
    "            #Save the model\n",
    "            current_weights.to_csv(\"Models/Vanilla_Perceptron_Weights_\"+str(iteration) +\"_.csv\", index = False)\n",
    "        \n",
    "        \n",
    "            \n",
    "            #reset the values\n",
    "            mistakes_in_epoch = 0\n",
    "            iteration += 1\n",
    "        \n",
    "        ##################################training ends here####################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "#         #Use the best model from validation results to predict for the test set.       \n",
    "#         best_weights = pd.read_csv(\"Models/Vanilla_Perceptron_Weights_\"+str(max_validation_accuracy_iteration_num) +\"_.csv\", sep = \",\", header = None)\n",
    "#         predictions_on_test_set = self.test(best_weights)\n",
    "        \n",
    "#         #Visualize the performance of the best model trained on three samples from the test set.\n",
    "#         random_indices_to_visualize = np.random.choice(len(self.features_test), 3, replace = False)    \n",
    "#                    \n",
    "            \n",
    "        \n",
    "    def validate(self, weights):\n",
    "        \n",
    "            mistakes_in_epoch = 0\n",
    "            \n",
    "            for idx in range(len(self.features_validation)):\n",
    "                \n",
    "                W_times_X = weights.dot(self.features_validation.iloc[idx,:])\n",
    "                \n",
    "                if self.targets_validation.iloc[idx,:].values[0] * W_times_X.values[0] <= 0.:\n",
    "                    \n",
    "                    #mistake occurred\n",
    "                    mistakes_in_epoch += 1\n",
    "                    \n",
    "                \n",
    "                #else the perceptron classified this sample correctly\n",
    "            \n",
    "            return mistakes_in_epoch\n",
    "        \n",
    "    def test(self, weights):\n",
    "        \n",
    "        predictions_on_test_set = []\n",
    "            \n",
    "        for idx in range(len(self.features_test)):\n",
    "\n",
    "            W_times_X = weights.dot(self.features_test.iloc[idx,:])\n",
    "\n",
    "            if self.targets_test[idx] * W_times_X <= 0:\n",
    "                predictions_on_test_set.append(5)\n",
    "            else:\n",
    "                prediction_on_test_set.append(3)\n",
    "\n",
    "\n",
    "            #else the perceptron classified this sample correctly\n",
    "\n",
    "        return predictions_on_test_set   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of random weights:  1.0\n",
      "Training Epoch:  0\n",
      "Validation Accuracy:  0.9337016574585635\n",
      "Training Epoch:  1\n",
      "Validation Accuracy:  0.94536525475752\n",
      "Training Epoch:  2\n",
      "Validation Accuracy:  0.9349294045426642\n",
      "Training Epoch:  3\n",
      "Validation Accuracy:  0.9484346224677717\n",
      "Training Epoch:  4\n",
      "Validation Accuracy:  0.9367710251688153\n",
      "Training Epoch:  5\n",
      "Validation Accuracy:  0.943523634131369\n",
      "Training Epoch:  6\n",
      "Validation Accuracy:  0.937998772252916\n",
      "Training Epoch:  7\n",
      "Validation Accuracy:  0.9386126457949663\n",
      "Training Epoch:  8\n",
      "Validation Accuracy:  0.9478207489257213\n",
      "Training Epoch:  9\n",
      "Validation Accuracy:  0.9392265193370166\n",
      "Training Epoch:  10\n",
      "Validation Accuracy:  0.9404542664211173\n",
      "Training Epoch:  11\n",
      "Validation Accuracy:  0.9429097605893186\n",
      "Training Epoch:  12\n",
      "Validation Accuracy:  0.9422958870472683\n",
      "Training Epoch:  13\n",
      "Validation Accuracy:  0.952731737262124\n",
      "Training Epoch:  14\n",
      "Validation Accuracy:  0.9459791282995703\n",
      "Training ends.\n"
     ]
    }
   ],
   "source": [
    "#Prepare data\n",
    "train_data_path = r\"C:\\Users\\Being_Aerys\\PycharmProjects\\Machine_Learning_Algorithms_Collection\\Supervised_Methods\\Perceptron\\Data\\pa2_train.csv\"\n",
    "validation_data_path = r\"C:\\Users\\Being_Aerys\\PycharmProjects\\Machine_Learning_Algorithms_Collection\\Supervised_Methods\\Perceptron\\Data\\pa2_valid.csv\"\n",
    "test_data_path = r\"C:\\Users\\Being_Aerys\\PycharmProjects\\Machine_Learning_Algorithms_Collection\\Supervised_Methods\\Perceptron\\Data\\pa2_test.csv\"\n",
    "\n",
    "preprocessing = Preprocessing()\n",
    "\n",
    "scaler, training_features, training_targets = preprocessing.preprocess_train_csv(train_data_path)\n",
    "\n",
    "validation_features, validation_targets = preprocessing.preprocess_validation_csv(scaler, validation_data_path)\n",
    "test_features = preprocessing.preprocess_test_csv(scaler, test_data_path)\n",
    "\n",
    "max_iterations_for_training = 25\n",
    "\n",
    "vanilla_perceptron = Vanilla_Perceptron(max_iterations_for_training, training_features, training_targets, validation_features, validation_targets, test_features)\n",
    "\n",
    "vanilla_perceptron.train()\n",
    "\n",
    "print(\"Training ends.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard SummaryWriter command\n",
    "#tensorboard --logdir \"C:\\Users\\Being_Aerys\\PycharmProjects\\Machine_Learning_Algorithms_Collection\\Supervised_Methods\\Perceptron\\runs\" --bind_all\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
