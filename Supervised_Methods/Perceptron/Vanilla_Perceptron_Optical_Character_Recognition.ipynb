{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Perceptron for Optical Character Recognition\n",
    "\n",
    "In the Optical Character Recognition (OCR) we seek to predict, for a given handwritten digit's image, the corresponding number. Here, we simplify the OCR into a binary classification task. Specifically, we consider predictions for only two numbers 3 and 5. \n",
    "## Training Data\n",
    "Includes 4888 rows (samples). Each sample/ row is in fact the target digit and a list of flattened gray-scale values from a 2d digital handwritten image with shape 28 × 28. -->  1 + 784  = 785 values. The first number is the digit’s label which is 3 or 5. The other 784 floating values are the grayscale values.\n",
    "## Validation Data\n",
    " Includes 1629 rows. Each row obeys the same format given for the train set. This set will be used to select your best trained model.\n",
    " ## Test Data\n",
    " Includes 1629 rows. Each row contains only 784 numbers. The label column (the first column in the trianing and the validatin data) is omitted from each row here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def convert_into_perceptron_binary_classes(self, df):\n",
    "        #change each row based on the condition\n",
    "        \n",
    "        final = []\n",
    "        \n",
    "        for idx in range(len(df)):\n",
    "            if df[idx] == 3:\n",
    "                final.append(1)\n",
    "            else:\n",
    "                final.append(-1)\n",
    "        return pd.DataFrame(final)\n",
    "        \n",
    "    \n",
    "    def preprocess_train_csv(self, path):\n",
    "        raw_data = pd.read_csv(path, sep = \",\", header = None)\n",
    "        features = raw_data.iloc[:, 1:]\n",
    "        \n",
    "        targets = raw_data.loc[:,0]\n",
    "        \n",
    "        #normalize the features\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(features)\n",
    "        scaled_features = scaler.transform(features) \n",
    "        \n",
    "        #bias of all ones\n",
    "        bias_feature = pd.DataFrame(np.ones(len(features)).reshape((len(features),1)))\n",
    "        \n",
    "        #stack the new bias feature to the left of the regular features using the axis argument\n",
    "        features_with_bias = pd.concat([bias_feature, features], axis = 1) #now has 785 features including the bias\n",
    "        \n",
    "        #convert the targets into -1 and +1 for perceptron operation        \n",
    "        targets = self.convert_into_perceptron_binary_classes(targets)\n",
    "        \n",
    "        return scaler, scaled_features, targets\n",
    "    \n",
    "    def preprocess_validation_csv(self, scaler, path):\n",
    "        raw_data = pd.read_csv(path, sep = \",\", header = None)\n",
    "        features = raw_data.iloc[:, 1:]\n",
    "        targets = raw_data.loc[:,0]\n",
    "        #transform the features using the parameters from the training data\n",
    "        scaled_features = scaler.transform(features)\n",
    "        \n",
    "        #bias of all ones\n",
    "        bias_feature = pd.DataFrame(np.ones(len(features)).reshape((len(features),1)))\n",
    "        \n",
    "        #stack the new bias feature to the left of the regular features using the axis argument\n",
    "        features_with_bias = pd.concat([bias_feature, features], axis = 1) #now has 785 features including the bias\n",
    "        \n",
    "        \n",
    "        \n",
    "        #convert the targets into -1 and +1 for perceptron operation\n",
    "        targets = self.convert_into_perceptron_binary_classes(targets)\n",
    "        \n",
    "        return  scaled_features, targets\n",
    "        \n",
    "        \n",
    "        \n",
    "    def preprocess_test_csv(self, scaler, path):\n",
    "        features = pd.read_csv(path, sep = \",\", header = None)\n",
    "        \n",
    "        \n",
    "        #transform the features using the parameters from the training data\n",
    "        scaled_features =  pd.DataFrame(scaler.transform(features))\n",
    "        #bias of all ones\n",
    "        bias_feature = pd.DataFrame(np.ones(len(scaled_features)).reshape((len(scaled_features),1)))\n",
    "        \n",
    "        #stack the new bias feature to the left of the regular features using the axis argument\n",
    "        features_with_bias = pd.concat([bias_feature, scaled_features], axis = 1) #now has 785 features including the bias\n",
    "        \n",
    "        return features_with_bias\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Vanilla_Perceptron(Perceptron):\n",
    "    \n",
    "    def __init__(self, max_iters, features_train, target_train, features_validation, targets_validation, features_test, targets_test):\n",
    "        self.features_train = features_train\n",
    "        self.targets_train = targets_train\n",
    "        self.features_validation = features_validation\n",
    "        self.targets_validation = targets_validation\n",
    "        self.features_test = features_test\n",
    "        self.targets_test = targets_test\n",
    "    \n",
    "    def train():\n",
    "        \n",
    "        #Initialize the weights and biases randomly\n",
    "        weights = pd.DataFrame(np.zeros(self.features.shape[1]))\n",
    "        biases = pd.DataFrame(np.zeros(self.features.shape[1]))\n",
    "        \n",
    "        iteration = 0\n",
    "        \n",
    "        while iteration < max_iters:\n",
    "            \n",
    "            #Get a shuffled list of indices\n",
    "            shuffled_indices = np.random.permutation(len(self.features_train))\n",
    "            \n",
    "            for idx in range(len(shuffled_indices)):\n",
    "                \n",
    "                W_times_X = weights.dot(self.features_train)\n",
    "                \n",
    "                if self.targets_train[idx] * W_times_X <= 0:\n",
    "                    \n",
    "                    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#Prepare data\n",
    "train_data_path = r\"C:\\Users\\Being_Aerys\\PycharmProjects\\Machine_Learning_Algorithms_Collection\\Supervised_Methods\\Perceptron\\Data\\pa2_train.csv\"\n",
    "validation_data_path = r\"C:\\Users\\Being_Aerys\\PycharmProjects\\Machine_Learning_Algorithms_Collection\\Supervised_Methods\\Perceptron\\Data\\pa2_valid.csv\"\n",
    "test_data_path = r\"C:\\Users\\Being_Aerys\\PycharmProjects\\Machine_Learning_Algorithms_Collection\\Supervised_Methods\\Perceptron\\Data\\pa2_test.csv\"\n",
    "\n",
    "preprocessing = Preprocessing()\n",
    "\n",
    "scaler, training_features, training_targets = preprocessing.preprocess_train_csv(train_data_path)\n",
    "validation_features, validation_targets = preprocessing.preprocess_validation_csv(scaler, validation_data_path)\n",
    "test_features = preprocessing.preprocess_test_csv(scaler, test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
